{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe4bb29-570a-4570-8adc-bde867921641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhanghanqing/anaconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import random\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "from eval_metric import *\n",
    "\n",
    "\n",
    "import transformers\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    BertTokenizer,\n",
    "    GPT2Tokenizer\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
    "from transformers import GPT2LMHeadModel, AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2b8a307-2093-43b5-9644-3bfd19700237",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.50949988628156 {'quagram': 0.33316810931363416, 'pentagram': 0.21513115687749343} 59.42864038725912\n",
      "93.12898592067371 {'quagram': 0.35413920919922554, 'pentagram': 0.22514594710652638} 46.89009688638192\n",
      "92.87153661778413 {'quagram': 0.35801516340204703, 'pentagram': 0.22389506027308811} 42.727942307138704\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_csv(\"./eval/generated_result_15_seed_42.csv\")\n",
    "gts = {}\n",
    "for index, row in enumerate(result.itertuples()):\n",
    "    text = row[-1].strip()\n",
    "    text = text.split(\".\")[0]\n",
    "    gts[str(row[1])] =  [text]\n",
    "coverage = evaluator_coverage(gts)\n",
    "self_bleu = evaluator_selfbleu(gts)\n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(coverage, self_bleu, ppls)\n",
    "\n",
    "\n",
    "result = pd.read_csv(\"./eval/generated_result_18_seed_42.csv\")\n",
    "gts = {}\n",
    "for index, row in enumerate(result.itertuples()):\n",
    "    text = row[-1].strip()\n",
    "    text = text.split(\".\")[0]\n",
    "    gts[str(row[1])] =  [text]\n",
    "coverage = evaluator_coverage(gts)\n",
    "self_bleu = evaluator_selfbleu(gts)\n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(coverage, self_bleu, ppls)\n",
    "\n",
    "\n",
    "result = pd.read_csv(\"./eval/generated_result_20_seed_42.csv\")\n",
    "gts = {}\n",
    "for index, row in enumerate(result.itertuples()):\n",
    "    text = row[-1].strip()\n",
    "    text = text.split(\".\")[0]\n",
    "    gts[str(row[1])] =  [text]\n",
    "coverage = evaluator_coverage(gts)\n",
    "self_bleu = evaluator_selfbleu(gts)\n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(coverage, self_bleu, ppls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3f92997d-d6e0-40a3-b33e-ec779a536e54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 {'quagram': 0.14588235361407634, 'pentagram': 0.07710747462297307} 41.81144101142883\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_csv(\"./eval/Text Sentence Context evaluation.csv\")\n",
    "gts = {}\n",
    "for index, row in enumerate(result.itertuples()):\n",
    "    text = row[2].strip()\n",
    "    text = row[1] +' '+text\n",
    "    # text = text.split(\".\")[0]\n",
    "    if row[3]== \"NRA-Mechturk-Sentence_sent\":\n",
    "        gts[str(row[1])] =  [text]\n",
    "        \n",
    "coverage = evaluator_coverage(gts)\n",
    "self_bleu = evaluator_selfbleu(gts)\n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(coverage, self_bleu, ppls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f98b5e-a8a9-4d9c-ae29-be19256d9b50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = pd.read_csv(\"./eval/Text Sentence Sense evaluation.csv\")\n",
    "gts = {}\n",
    "for index, row in enumerate(result.itertuples()):\n",
    "    text = row[1].strip()\n",
    "    # text = text.split(\".\")[0]\n",
    "    if row[2]== alg_1:\n",
    "        gts[str(row[1])] =  [text]\n",
    "        \n",
    "coverage = evaluator_coverage(gts)\n",
    "self_bleu = evaluator_selfbleu(gts)\n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(coverage, self_bleu, ppls)\n",
    "\n",
    "result = pd.read_csv(\"./eval/Text Sentence Sense evaluation.csv\")\n",
    "gts = {}\n",
    "for index, row in enumerate(result.itertuples()):\n",
    "    text = row[1].strip()\n",
    "    # text = text.split(\".\")[0]\n",
    "    if row[2]==alg_2:\n",
    "        gts[str(row[1])] =  [text]\n",
    "        \n",
    "coverage = evaluator_coverage(gts)\n",
    "self_bleu = evaluator_selfbleu(gts)\n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(coverage, self_bleu, ppls)\n",
    "\n",
    "result = pd.read_csv(\"./eval/Text Sentence Sense evaluation.csv\")\n",
    "gts = {}\n",
    "for index, row in enumerate(result.itertuples()):\n",
    "    text = row[1].strip()\n",
    "    # text = text.split(\".\")[0]\n",
    "    if row[2]==alg_3:\n",
    "        gts[str(row[1])] =  [text]\n",
    "        \n",
    "coverage = evaluator_coverage(gts)\n",
    "self_bleu = evaluator_selfbleu(gts)\n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(coverage, self_bleu, ppls)\n",
    "\n",
    "\n",
    "result = pd.read_csv(\"./eval/Text Sentence Sense evaluation.csv\")\n",
    "gts = {}\n",
    "for index, row in enumerate(result.itertuples()):\n",
    "    text = row[1].strip()\n",
    "    # text = text.split(\".\")[0]\n",
    "    if row[2]==alg_4:\n",
    "        gts[str(row[1])] =  [text]\n",
    "        \n",
    "coverage = evaluator_coverage(gts)\n",
    "self_bleu = evaluator_selfbleu(gts)\n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(coverage, self_bleu, ppls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eb1bb572-0ebd-4b3f-a9d9-7393f9a0c2e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean offset is:-0.3760855043420168, standard div:0.7974155534454002\n",
      "mean offset is:-0.45624582498330035, standard div:0.7579278417675885\n",
      "mean offset is:-0.5858383433533731, standard div:0.7067378989461931\n",
      "mean offset is:-0.6145624582498321, standard div:0.7149111507612138\n",
      "mean offset is:-0.7074148296593172, standard div:0.7365443806846922\n",
      "mean offset is:-0.7147628590514365, standard div:0.6961320949564399\n",
      "mean offset is:-0.774215096860388, standard div:0.660013724708826\n",
      "mean offset is:-0.9438877755511008, standard div:0.8057640664070113\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path =\"/home2/zhanghanqing/pretrained_model/gpt2/large\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "result = pd.read_csv(\"./eval/sentence_length/generated_result_10_seed_42.csv\")\n",
    "len_gts = []\n",
    "for index, row in enumerate(result.itertuples()):\n",
    "    text = row[-2].strip()\n",
    "    text = text.split(\".\")[0]+'.'\n",
    "    len_gts.append(len(tokenizer.encode(text)))\n",
    "\n",
    "m= mean(len_gts)-10\n",
    "std = np.std(len_gts, ddof=1)\n",
    "print(f\"mean offset is:{m}, standard div:{std}\")\n",
    "\n",
    "result = pd.read_csv(\"./eval/sentence_length/generated_result_12_seed_42.csv\")\n",
    "len_gts = []\n",
    "for index, row in enumerate(result.itertuples()):\n",
    "    text = row[-2].strip()\n",
    "    text = text.split(\".\")[0]+'.'\n",
    "    len_gts.append(len(tokenizer.encode(text)))\n",
    "\n",
    "m= mean(len_gts)-12\n",
    "std = np.std(len_gts, ddof=1)\n",
    "print(f\"mean offset is:{m}, standard div:{std}\")\n",
    "\n",
    "\n",
    "result = pd.read_csv(\"./eval/sentence_length/generated_result_14_seed_42.csv\")\n",
    "len_gts = []\n",
    "for index, row in enumerate(result.itertuples()):\n",
    "    text = row[-2].strip()\n",
    "    text = text.split(\".\")[0]+'.'\n",
    "    len_gts.append(len(tokenizer.encode(text)))\n",
    "\n",
    "m= mean(len_gts)-14\n",
    "std = np.std(len_gts, ddof=1)\n",
    "print(f\"mean offset is:{m}, standard div:{std}\")\n",
    "\n",
    "\n",
    "result = pd.read_csv(\"./eval/sentence_length/generated_result_16_seed_42.csv\")\n",
    "len_gts = []\n",
    "for index, row in enumerate(result.itertuples()):\n",
    "    text = row[-2].strip()\n",
    "    text = text.split(\".\")[0]+'.'\n",
    "    len_gts.append(len(tokenizer.encode(text)))\n",
    "\n",
    "m= mean(len_gts)-16\n",
    "std = np.std(len_gts, ddof=1)\n",
    "print(f\"mean offset is:{m}, standard div:{std}\")\n",
    "\n",
    "\n",
    "result = pd.read_csv(\"./eval/sentence_length/generated_result_18_seed_42.csv\")\n",
    "len_gts = []\n",
    "for index, row in enumerate(result.itertuples()):\n",
    "    text = row[-2].strip()\n",
    "    text = text.split(\".\")[0]+'.'\n",
    "    len_gts.append(len(tokenizer.encode(text)))\n",
    "    \n",
    "\n",
    "m= mean(len_gts)-18\n",
    "std = np.std(len_gts, ddof=1)\n",
    "print(f\"mean offset is:{m}, standard div:{std}\")\n",
    "\n",
    "\n",
    "result = pd.read_csv(\"./eval/sentence_length/generated_result_20_seed_42.csv\")\n",
    "len_gts = []\n",
    "for index, row in enumerate(result.itertuples()):\n",
    "    text = row[-2].strip()\n",
    "    text = text.split(\".\")[0]+'.'\n",
    "    len_gts.append(len(tokenizer.encode(text)))\n",
    "    \n",
    "\n",
    "m= mean(len_gts)-20\n",
    "std = np.std(len_gts, ddof=1)\n",
    "print(f\"mean offset is:{m}, standard div:{std}\")\n",
    "\n",
    "\n",
    "result = pd.read_csv(\"./eval/sentence_length/generated_result_22_seed_42.csv\")\n",
    "len_gts = []\n",
    "for index, row in enumerate(result.itertuples()):\n",
    "    text = row[-2].strip()\n",
    "    text = text.split(\".\")[0]+'.'\n",
    "    len_gts.append(len(tokenizer.encode(text)))\n",
    "    \n",
    "\n",
    "m= mean(len_gts)-22\n",
    "std = np.std(len_gts, ddof=1)\n",
    "print(f\"mean offset is:{m}, standard div:{std}\")\n",
    "\n",
    "\n",
    "result = pd.read_csv(\"./eval/sentence_length/generated_result_24_seed_42.csv\")\n",
    "len_gts = []\n",
    "for index, row in enumerate(result.itertuples()):\n",
    "    text = row[-2].strip()\n",
    "    text = text.split(\".\")[0]+'.'\n",
    "    len_gts.append(len(tokenizer.encode(text)))\n",
    "    \n",
    "\n",
    "m= mean(len_gts)-24\n",
    "std = np.std(len_gts, ddof=1)\n",
    "print(f\"mean offset is:{m}, standard div:{std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a52542a-cb86-45bd-8db1-012034933775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 {'quagram': 0.0901401388523252, 'pentagram': 0.056976261110941735} 65.53158626337161\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_csv(\"./eval/c2gen2.csv\")\n",
    "gts = {}\n",
    "for index, row in enumerate(result.itertuples()):\n",
    "    text = row[-1].strip()\n",
    "    if text.endswith('.'):\n",
    "        text= text[:-1].strip()+'.'\n",
    "        \n",
    "    gts[str(row[1])] =  [text]\n",
    "        \n",
    "coverage = evaluator_coverage(gts)\n",
    "self_bleu = evaluator_selfbleu(gts)\n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(coverage, self_bleu, ppls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b19e9ef-8e21-4141-a371-af22ec541be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 {'quagram': 0.25193776421498837, 'pentagram': 0.16109413054717062} 59.07012661901014\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_csv(\"./eval/commongen500.csv\")\n",
    "gts = {}\n",
    "for index, row in enumerate(result.itertuples()):\n",
    "    if pd.isnull(row[-2]):\n",
    "        continue\n",
    "    \n",
    "    text = row[-2].strip()\n",
    "    if text.endswith('.'):\n",
    "        text= text[:-1].strip()+'.'\n",
    "        \n",
    "    gts[str(row[1])] =  [text]\n",
    "        \n",
    "coverage = evaluator_coverage(gts)\n",
    "self_bleu = evaluator_selfbleu(gts)\n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(coverage, self_bleu, ppls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5c3992f6-77b3-4718-b8bb-7e8de2c5e984",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 {'quagram': 0.10786878461075379, 'pentagram': 0.05196704179917172} 46.703285063155974\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_csv(\"./eval/c2gen2.csv\")\n",
    "gts = {}\n",
    "for index, row in enumerate(result.itertuples()):\n",
    "    text = row[-1].strip()\n",
    "    if text.endswith('.'):\n",
    "        text= text[:-1].strip()+'.'\n",
    "        \n",
    "        \n",
    "    text  = row[-3]+' '+text\n",
    "\n",
    "    gts[str(row[1])] =  [text]\n",
    "        \n",
    "coverage = evaluator_coverage(gts)\n",
    "self_bleu = evaluator_selfbleu(gts)\n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(coverage, self_bleu, ppls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6ab8657c-7ec2-4742-81a6-883622ec8bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.375 {'quagram': 0.1245602667904008, 'pentagram': 0.0615776000369825} 46.204694824989396\n",
      "95.9 {'quagram': 0.14250350551547447, 'pentagram': 0.07189511450884259} 46.181719192350755\n",
      "95.15 {'quagram': 0.15461118101001745, 'pentagram': 0.07857545907104785} 46.180738583959716\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_csv(\"./eval/generated_result_15_seed_1.csv\")\n",
    "gts = {}\n",
    "for index, row in enumerate(result.itertuples()):\n",
    "    text = row[3].strip().split(\".\")[0]+'.'\n",
    "    text  = row[2]+' '+text\n",
    "    gts[str(row[1])] =  [text]\n",
    "        \n",
    "coverage = evaluator_coverage(gts)\n",
    "self_bleu = evaluator_selfbleu(gts)\n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(coverage, self_bleu, ppls)\n",
    "\n",
    "result = pd.read_csv(\"./eval/generated_result_18_seed_1.csv\")\n",
    "gts = {}\n",
    "for index, row in enumerate(result.itertuples()):\n",
    "    text = row[3].strip().split(\".\")[0]+'.'\n",
    "    text  = row[2]+' '+text\n",
    "\n",
    "    gts[str(row[1])] =  [text]\n",
    "        \n",
    "coverage = evaluator_coverage(gts)\n",
    "self_bleu = evaluator_selfbleu(gts)\n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(coverage, self_bleu, ppls)\n",
    "\n",
    "result = pd.read_csv(\"./eval/generated_result_20_seed_1.csv\")\n",
    "gts = {}\n",
    "for index, row in enumerate(result.itertuples()):\n",
    "    text = row[3].strip().split(\".\")[0]+'.'\n",
    "    text  = row[2]+' '+text\n",
    "\n",
    "    # print(text)\n",
    "    gts[str(row[1])] =  [text]\n",
    "        \n",
    "coverage = evaluator_coverage(gts)\n",
    "self_bleu = evaluator_selfbleu(gts)\n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(coverage, self_bleu, ppls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df569a-e6c7-4814-87af-5d8fd54f4a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py39]",
   "language": "python",
   "name": "conda-env-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
