{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "5c27bbae-fdcf-4e31-ba4c-5e39548022f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import transformers\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    BertTokenizer,\n",
    "    GPT2Tokenizer\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
    "from transformers import GPT2LMHeadModel, AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "model_name_or_path = \"/home2/zhanghanqing/pretrained_model/gpt2/large\"\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "            model_name_or_path)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "7ac9e34a-0b4f-47ae-a4db-18289f1c9198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_metric import cal_ppl_bygpt2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "text = [\"I love you\", \"love pretrain hello\"]\n",
    "\n",
    "ppl = cal_ppl_bygpt2(tokenizer, model, 20, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c7204bc5-909f-4313-bf68-4eaf66c084fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0.dev20221024+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "708a8258-75a3-4620-86ac-d4a5683b5b41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhanghanqing/anaconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import copy\n",
    "from typing import Optional, Any, Union, Callable\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import MultiheadAttention\n",
    "from torch.nn import Dropout\n",
    "from torch.nn import Linear\n",
    "from torch.nn import LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "883714ac-72c2-4bb6-9d65-ff8d4ceea099",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformerDecoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0md_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnhead\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdim_feedforward\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdropout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mactivation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mrelu\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x7f9f4525d3a0\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlayer_norm_eps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnorm_first\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.\n",
       "This standard decoder layer is based on the paper \"Attention Is All You Need\".\n",
       "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
       "Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
       "Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
       "in a different way during application.\n",
       "\n",
       "Args:\n",
       "    d_model: the number of expected features in the input (required).\n",
       "    nhead: the number of heads in the multiheadattention models (required).\n",
       "    dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
       "    dropout: the dropout value (default=0.1).\n",
       "    activation: the activation function of the intermediate layer, can be a string\n",
       "        (\"relu\" or \"gelu\") or a unary callable. Default: relu\n",
       "    layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n",
       "    batch_first: If ``True``, then the input and output tensors are provided\n",
       "        as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
       "    norm_first: if ``True``, layer norm is done prior to self attention, multihead\n",
       "        attention and feedforward operations, respectively. Otherwise it's done after.\n",
       "        Default: ``False`` (after).\n",
       "\n",
       "Examples::\n",
       "    >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
       "    >>> memory = torch.rand(10, 32, 512)\n",
       "    >>> tgt = torch.rand(20, 32, 512)\n",
       "    >>> out = decoder_layer(tgt, memory)\n",
       "\n",
       "Alternatively, when ``batch_first`` is ``True``:\n",
       "    >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8, batch_first=True)\n",
       "    >>> memory = torch.rand(32, 10, 512)\n",
       "    >>> tgt = torch.rand(32, 20, 512)\n",
       "    >>> out = decoder_layer(tgt, memory)\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/transformer.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     Transformer_Decoder\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?nn.TransformerDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b8572ad-e428-447b-91ad-9fbfd9cf7d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhanghanqing/anaconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from adapters.untitled  import Transformer_Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2f45e8c-14d2-4e09-b7d8-e163fa29dfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c21129b-387c-40c0-8e99-4caff433f34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "decoder_layer = Transformer_Decoder(d_model=512, nhead=8, batch_first=True)\n",
    "memory = torch.rand(32, 10, 512)\n",
    "tgt = torch.rand(32, 20, 512)\n",
    "out = decoder_layer(tgt, tgt, memory)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9a04fc76-6c05-466a-96c8-9ee9ae5135c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 15496,    11,   616,  3290,   318,   257,  1310,  1643,   286,\n",
      "           257,  2356,   287,   262,   840,    11,   475,   339,   338,   257],\n",
      "        [ 8206,   290,  2456,  2291,    25, 16569,    11,  9280,    11,  1620,\n",
      "            11,  3800,    11,  5806,    13,  7253,  5270,    25,   352,    12,\n",
      "            17,  2745,    13,   198,   198,    16,    12,    17,  2745,    13]])\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is a little bit of a pain in the ass, but he's a\n",
      "Text and words include: costume, dance, perform, stage, wear. Start generation: 1-2 weeks.\n",
      "\n",
      "1-2 weeks.\n"
     ]
    }
   ],
   "source": [
    "tokenizer.padding_side = \"left\" \n",
    "tokenizer.pad_token = tokenizer.eos_token # to avoid an error\n",
    "\n",
    "sentences = [\"Hello, my dog is a little\",\n",
    "            \"Text and words include: costume, dance, perform, stage, wear. Start generation:\"\n",
    "            ]\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "output_sequences = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    num_beams=3,\n",
    "    do_sample=False, \n",
    "    max_length=30 # disable sampling to test if batching affects output\n",
    ")\n",
    "print(output_sequences)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    print(tokenizer.decode(output_sequences[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6d40d148-5be5-4729-a416-f5e561af7530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "d = torch.tensor([\n",
    "[1,2,3,4],\n",
    "[1,2,3,4]\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f627624f-ea4b-439f-acfb-891a1047e915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 4, 6, 8],\n",
      "        [2, 4, 6, 8]])\n"
     ]
    }
   ],
   "source": [
    "c=d+d\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "669cdbe3-b62a-4339-838f-e02234004362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "import nltk.translate.bleu_score as nltkbleu\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import spacy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def nltk_bleu(refs, pred, n):\n",
    "    \"\"\"\n",
    "    一般smoothing_function选择默认即可；\n",
    "    默认n=4\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = [v[0] for k,v in res.items()]\n",
    "    references = [v for k,v in gts.items()]\n",
    "    \n",
    "    refs=[]\n",
    "    \n",
    "    refs = [tokenizer(references) for ref in refs]\n",
    "    pred = tokenizer(predictions)\n",
    "    \n",
    "    weights = [1 / n for _ in range(n)]\n",
    "    score = sentence_bleu(\n",
    "        refs,\n",
    "        pred,\n",
    "        smoothing_function=nltkbleu.SmoothingFunction().method7,\n",
    "        weights=weights\n",
    "    )\n",
    "    print(score)\n",
    "    \n",
    "\n",
    "\n",
    "def calculateSelfBLEU(texts, ngram=5):\n",
    "    if len(texts) == 1:\n",
    "        return 0\n",
    "\n",
    "    spacyTexts = list(nlp.pipe(texts))\n",
    "    textsSplits = np.array([[token.text for token in t] for t in spacyTexts], dtype=object)\n",
    "\n",
    "    arange = np.arange(len(textsSplits))\n",
    "    weights = tuple((1. / ngram for _ in range(ngram)))\n",
    "\n",
    "    pool = Pool(os.cpu_count())\n",
    "    bleus = list()\n",
    "    for idx, candidate in enumerate(textsSplits):\n",
    "        reference = textsSplits[arange != idx].tolist()\n",
    "        bleus.append(pool.apply_async(calcBleu, args=(reference, candidate, weights)))\n",
    "\n",
    "    for idx, b in tqdm(enumerate(bleus), total=len(bleus)):\n",
    "        bleus[idx] = b.get()\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    return np.mean(bleus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9e593f4-65eb-4c82-8b96-7bcf576f82bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = {\n",
    "\"1\":[\"Today is a nice days\"],\n",
    "\"2\":[\"I like  huggingface, it is easy to use\"],\n",
    "\"3\":[\"big model is changing the worlds\"]\n",
    "}\n",
    "\n",
    "gts = {\n",
    "\"1\":[\"Today is a nice day\"],\n",
    "\"2\":[\"I like huggingface, it is easy to use\"],\n",
    "\"3\":[\"big model is changing the worlds\"]\n",
    "}\n",
    "\n",
    "# ref = [v for k, v in reference.items()]\n",
    "\n",
    "# cand = [v for k, v in text.items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "885edf37-b05e-4fba-85fb-ece4b40bb309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': ['today is a nice day'], '2': ['i like huggingface it is easy to use'], '3': ['big model is changing the worlds']} {'1': ['today is a nice days'], '2': ['i like huggingface it is easy to use'], '3': ['big model is changing the world']}\n"
     ]
    }
   ],
   "source": [
    "from speaksee import evaluation\n",
    "\n",
    "def tokenize(dict):\n",
    "    for key in dict:\n",
    "        new_sentence_list = []\n",
    "        for sentence in dict[key]:\n",
    "            a = ''\n",
    "            for token in nlp(sentence):\n",
    "                a += token.text\n",
    "                a += ' '\n",
    "            new_sentence_list.append(a.rstrip())\n",
    "        dict[key] = new_sentence_list\n",
    "\n",
    "    return dict\n",
    "\n",
    "# gts = tokenize(gts)\n",
    "# gen = tokenize(gen)\n",
    "\n",
    "gts = evaluation.PTBTokenizer.tokenize(gts)\n",
    "gen = evaluation.PTBTokenizer.tokenize(gen)\n",
    "\n",
    "\n",
    "print(gts,gen)\n",
    "# val_bleu, _ = evaluation.Bleu(n=4).compute_score(gts, gen)\n",
    "# method = ['Blue_1', 'Bleu_2', 'Bleu_3', 'Bleu_4']\n",
    "# metric_dict = {}\n",
    "# for metric, score in zip(method, val_bleu):\n",
    "#     metric_dict[metric] = {'entire': score * 100}\n",
    "#     print('%s %.2f' % (metric, score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf07bc0c-3c9e-402a-bf0d-449519e59481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['today is a nice day'] ['today is a nice days']\n",
      "['i like huggingface it is easy to use'] ['i like huggingface it is easy to use']\n",
      "['big model is changing the worlds'] ['big model is changing the world']\n",
      "NIST 3.85\n",
      "BLEU 85.32\n"
     ]
    }
   ],
   "source": [
    "from eval_metric import BLEUScore, NISTScore\n",
    "\n",
    "bleu = BLEUScore()\n",
    "nist = NISTScore()\n",
    "\n",
    "for sents_ref, sent_sys in zip(gts.values(), gen.values()):\n",
    "    print(sents_ref, sent_sys)\n",
    "    \n",
    "    bleu.append(sent_sys[0], sents_ref)\n",
    "    nist.append(sent_sys[0], sents_ref)\n",
    "    \n",
    "print(\"NIST %.2f\" % (nist.score()))\n",
    "print(\"BLEU %.2f\" % (bleu.score() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "fa67331a-7bc0-41bb-91f3-faefb90b26e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Today', 'is', 'a', 'nice', 'day']]\n",
      "[['I', 'like', 'huggingface,', 'it', 'is', 'easy', 'to', 'use']]\n",
      "[['big', 'model', 'is', 'changing', 'the', 'world']]\n",
      "[0.7745966692414834, 0.6976750600527641, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bigram': 0.8240905764314158,\n",
       " 'trigram': 0.7785575236143242,\n",
       " 'quagram': 0.7311587009123848}"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fast_bleu import BLEU, SelfBLEU\n",
    "import numpy as np\n",
    "\n",
    "evaluator_bleu(text, reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "305be2c2-1146-4c99-9b7d-2814cf38bf73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['big', 'model', 'is', 'changing', 'the', 'world']"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference[\"3\"][0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1343487-8351-4155-a73b-0b7f7bf65394",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,\"mask\",\"mask\",5,6,7,8,\"mask\",10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b4cb2f76-65b7-43c8-8df0-32d891a2b007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1, 2]\n",
      "[1, 2, 3]\n",
      "[1, 2, 3, 'mask']\n",
      "[1, 2, 3, 'mask']\n",
      "[1, 2, 3, 'mask', 5]\n",
      "[1, 2, 3, 'mask', 5, 6]\n",
      "[1, 2, 3, 'mask', 5, 6, 7]\n",
      "[1, 2, 3, 'mask', 5, 6, 7, 8]\n",
      "[1, 2, 3, 'mask', 5, 6, 7, 8, 'mask']\n",
      "[1, 2, 3, 'mask', 5, 6, 7, 8, 'mask', 10]\n"
     ]
    }
   ],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c16fb73f-9c97-4e65-8ea4-99e6eb15d13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 'mask', 5, 6, 7, 8, 'mask', 10]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d5ad63eb-f138-4a44-a1a1-bdfdb838d794",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (449295825.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [81]\u001b[0;36m\u001b[0m\n\u001b[0;31m    a 6\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "a 6\n",
    "if a%6 < 6:\n",
    "    print(\"hello\")\n",
    "elif a %6==1:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9055129d-d955-43cf-8226-dd0b6c07a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_set_input_ids =[1,2,3,4,5,6,7,7,9,0]\n",
    "length = len(concept_set_input_ids)\n",
    "sentinal_count = max(int(length * 0.2), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4d50ebcc-733a-4c8b-8bca-c3947e34a6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentinal_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ea03acdf-f2cd-4686-b535-9559ee754a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1026dbfe-1b87-4ece-9560-1fb1822736e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py39]",
   "language": "python",
   "name": "conda-env-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
