{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c27bbae-fdcf-4e31-ba4c-5e39548022f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import transformers\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    BertTokenizer,\n",
    "    GPT2Tokenizer\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
    "from transformers import GPT2LMHeadModel, AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "model_name_or_path = \"/home2/zhanghanqing/pretrained_model/gpt2/large\"\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "            model_name_or_path)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89b42080-dd73-4537-9202-b53d904b8f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Include the words [eat, hay, horse]: The horse was hungry and went down to the other animal in the barn to eat hay. 2. Include the words [computer, scientist, office]: In the office at this time of day, one could see a large group of enthusiastic scientists sitting by their computers. 3. Include the words [lawn, mow, noon, day]: David had to mow the lawn by noon, he had worked all day and started to get really exhausted. 4. Include the words [dog, cat, dirt, floor, window]: The floor was full of dirt after the dog had chased the cat out of the window.\n"
     ]
    }
   ],
   "source": [
    "data  = \"1. Include the words [eat, hay, horse]: The horse was hungry and went down to the other animal in the barn to eat hay. \\\n",
    "2. Include the words [computer, scientist, office]: In the office at this time of day, one could see a large group of enthusiastic scientists sitting by their computers. \\\n",
    "3. Include the words [lawn, mow, noon, day]: David had to mow the lawn by noon, he had worked all day and started to get really exhausted. \\\n",
    "4. Include the words [dog, cat, dirt, floor, window]: The floor was full of dirt after the dog had chased the cat out of the window.\"\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7a23761-f38e-45b4-9d5d-ea04a0896f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [], 'attention_mask': []}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ''\n",
    "tokenizer(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae6f6217-8767-4a5c-9e78-68af520afdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a3\n"
     ]
    }
   ],
   "source": [
    "print(f\"a{len([1,3,4])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9a04fc76-6c05-466a-96c8-9ee9ae5135c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 15496,    11,   616,  3290,   318,   257,  1310,  1643,   286,\n",
      "           257,  2356,   287,   262,   840,    11,   475,   339,   338,   257],\n",
      "        [ 8206,   290,  2456,  2291,    25, 16569,    11,  9280,    11,  1620,\n",
      "            11,  3800,    11,  5806,    13,  7253,  5270,    25,   352,    12,\n",
      "            17,  2745,    13,   198,   198,    16,    12,    17,  2745,    13]])\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is a little bit of a pain in the ass, but he's a\n",
      "Text and words include: costume, dance, perform, stage, wear. Start generation: 1-2 weeks.\n",
      "\n",
      "1-2 weeks.\n"
     ]
    }
   ],
   "source": [
    "tokenizer.padding_side = \"left\" \n",
    "tokenizer.pad_token = tokenizer.eos_token # to avoid an error\n",
    "\n",
    "sentences = [\"Hello, my dog is a little\",\n",
    "            \"Text and words include: costume, dance, perform, stage, wear. Start generation:\"\n",
    "            ]\n",
    "\n",
    "\n",
    "data  = \"1. Include the words [eat, hay, horse]: The horse was hungry and went down to the other animal in the barn to eat hay.  \\\\\n",
    "2. Include the words [computer, scientist, office]: In the office at this time of day, one could see a large group of enthusiastic scientists sitting by their computers. \\\\\n",
    "3. Include the words [lawn, mow, noon, day]: David had to mow the lawn by noon, he had worked all day and started to get really exhausted.\\\\\n",
    "4. Include the words [soccer, sit, sofa, chance]: The whole group got the chance to sit down on the sofa before the soccer game. \\\\\n",
    "5. Include the words [dog, cat, dirt, floor, window]: The floor was full of dirt after the dog had chased the cat out of the window.\"\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "output_sequences = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    num_beams=3,\n",
    "    do_sample=False, \n",
    "    max_length=30 # disable sampling to test if batching affects output\n",
    ")\n",
    "print(output_sequences)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    print(tokenizer.decode(output_sequences[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d40d148-5be5-4729-a416-f5e561af7530",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data from /home2/zhanghanqing/formatted_wikipedia ...\n",
      "20001 100001 0\n",
      "data num is: 120002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:42<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the record is: 98516\n",
      "total data number is: 98516\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import json\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import random\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def encoder_data_target(data):\n",
    "    \n",
    "    nn = ['VERB', 'NOUN'] \n",
    "    nlp = spacy.load('en_core_web_sm')# 加载预训练模型\n",
    "    res = []\n",
    "    \n",
    "    for line in data:\n",
    "        line_content  = ' '.join(line[:-1])\n",
    "        target = line[-1]\n",
    "        \n",
    "        if len(target.split())<6:\n",
    "            continue\n",
    "                \n",
    "        doc = nlp(target.lower())\n",
    "        tokens = [token for token in doc]           # 将句子切分成单词\n",
    "        pos = [token.pos_ for token in doc]         # 词性标注\n",
    "        lem = [token.lemma_ for token in doc]       # 词性还原\n",
    "        \n",
    "        sen = []\n",
    "        for t,p,l in zip(tokens, pos, lem):\n",
    "            if (p in nn) and (len(str(l))>=3) and ('-'  not in str(l)) and ('.' not in str(l)):\n",
    "                sen.append(str(l))\n",
    "                \n",
    "        sen = list(set(sen))\n",
    "        \n",
    "        if len(sen)<4:\n",
    "            continue\n",
    "            \n",
    "        if len(sen)>5:\n",
    "            \n",
    "            sen =  random.sample(sen, 5)\n",
    "            \n",
    "        random.shuffle(sen)\n",
    " \n",
    "        res.append({\n",
    "                    \"content\":line_content,\n",
    "                    \"target\":target,\n",
    "                    \"keywords\": '#'.join(sen)})\n",
    "        \n",
    "    return res\n",
    "\n",
    "\n",
    "def encoder_data(data):\n",
    "    \n",
    "    nn = ['VERB', 'NOUN'] \n",
    "    nlp = spacy.load('en_core_web_sm')# 加载预训练模型\n",
    "\n",
    "    res = []\n",
    "    \n",
    "    for line in data:\n",
    "        \n",
    "        # cut_pos = random.randint(8,25)\n",
    "        line = line.strip(' \\n\\t\\r),\\\\')\n",
    "        \n",
    "        \n",
    "        # if line.endswith('.'):\n",
    "        #     line= line[:-1].strip()\n",
    "        \n",
    "        \n",
    "        if len(line)<3:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        if line[0].islower():\n",
    "            continue\n",
    "        \n",
    "        if \"amp\" in line:\n",
    "            continue\n",
    "        \n",
    "        split_line= line.split()\n",
    "        \n",
    "        # if cut_pos>len(split_line) or len(split_line)>150 or len(split_line)<3:\n",
    "        #     continue\n",
    "        \n",
    "        # line_content  = ' '.join(split_line[:-cut_pos])\n",
    "        # target = ' '.join(split_line[-cut_pos:])\n",
    "        \n",
    "        line_content=''\n",
    "        target = line\n",
    "                \n",
    "        doc = nlp(target.lower())\n",
    "        tokens = [token for token in doc]           # 将句子切分成单词\n",
    "        pos = [token.pos_ for token in doc]         # 词性标注\n",
    "        lem = [token.lemma_ for token in doc]       # 词性还原\n",
    "        \n",
    "        sen = []\n",
    "        for t,p,l in zip(tokens, pos, lem):\n",
    "            if (p in nn) and (len(str(l))>=3) and ('-'  not in str(l)) and ('.' not in str(l)):\n",
    "                sen.append(str(l))\n",
    "                \n",
    "        sen = list(set(sen))\n",
    "        \n",
    "        if len(sen)<4:\n",
    "            continue\n",
    "            \n",
    "        if len(sen)>5:\n",
    "            \n",
    "            sen =  random.sample(sen, random.randint(4,5))\n",
    "            \n",
    "        random.shuffle(sen)\n",
    " \n",
    "        res.append({\n",
    "                    \"content\":line_content,\n",
    "                    \"target\":target,\n",
    "                    \"keywords\": '#'.join(sen)})\n",
    "        \n",
    "    return res \n",
    "\n",
    "json_path = '/home2/zhanghanqing/formatted_wikipedia'\n",
    "\n",
    "out_file_train = os.path.join('./data/', 'wiki_train.json')\n",
    "out_file_train = open(out_file_train, 'w', encoding='utf8')\n",
    "\n",
    "\n",
    "out_file_val = os.path.join('./data/', 'wiki_val.json')\n",
    "out_file_val = open(out_file_val, 'w', encoding='utf8')\n",
    "\n",
    "\n",
    "#######################################wiki dataset########################################\n",
    "\n",
    "print(\"reading data from %s ...\" % json_path)\n",
    "\n",
    "filenames = []                \n",
    "filenames += glob(os.path.join(json_path,'wiki**.format'))\n",
    "data =  []\n",
    "record = []\n",
    "\n",
    "for data_file in filenames:\n",
    "    data +=[x for x in Path(data_file).open().readlines()[-40000:]]\n",
    "    \n",
    "d_0_15 = 0  #6w\n",
    "d_15_20 = 0 #3w\n",
    "d_20=0  #1w\n",
    "    \n",
    "filter_data = []  \n",
    "\n",
    "for d in data:\n",
    "    l_d = len(d.split())\n",
    "    \n",
    "    if d_0_15>20000 and d_15_20>100000 and d_20>40000:\n",
    "        break\n",
    "    \n",
    "    if l_d>=10 and l_d<15  and d_0_15<=20000:\n",
    "        d_0_15 +=1\n",
    "        filter_data.append(d)\n",
    "        \n",
    "    elif l_d>=15 and l_d<=20 and d_15_20<=100000:\n",
    "        d_15_20 +=1\n",
    "        filter_data.append(d)\n",
    "        \n",
    "    # elif l_d>20 and l_d<=32 and d_20<=40000:\n",
    "    #     d_20 += 1\n",
    "    #     filter_data.append(d)\n",
    "             \n",
    "print(d_0_15,d_15_20, d_20)\n",
    "        \n",
    "data = filter_data\n",
    "print(\"data num is:\", len(data))\n",
    "\n",
    "n = int(len(data)/48)\n",
    "data_list =[data[i:i + n] for i in range(0, len(data), n)]\n",
    "\n",
    "with Pool(48) as p:\n",
    "\n",
    "    data =list(tqdm(p.imap(encoder_data, data_list), total=len(data_list)))\n",
    "\n",
    "record = [item for subl in data for item in subl]\n",
    "\n",
    "print(\"the record is:\",len(record))\n",
    "\n",
    "#######################################roc dataset########################################\n",
    "# roc = pd.read_csv(\"../data/roc/roc.csv\")\n",
    "# data = []\n",
    "# for row in roc.itertuples():\n",
    "#     # if random.randint(1,5)%5==1:\n",
    "#     cut_pos = random.randint(3,5)\n",
    "#     # else:\n",
    "#     #     cut_pos = 5\n",
    "#     context = [row[i+3] for i in range(cut_pos)]\n",
    "#     data.append(context)\n",
    "    \n",
    "# random.shuffle(data)\n",
    "\n",
    "\n",
    "# n = int(len(data)/48)\n",
    "# data_list =[data[i:i + n] for i in range(0, len(data), n)]\n",
    "\n",
    "# with Pool(48) as p:\n",
    "\n",
    "#     data =list(tqdm(p.imap(encoder_data_target, data_list), total=len(data_list)))\n",
    "\n",
    "# record += [item for subl in data for item in subl]\n",
    "\n",
    "#######################################hc dataset########################################\n",
    "# data_file = \"../data/hc/train.src\"\n",
    "# data =[eval(x.strip(' \\n\\t\\r),\\\\')) for x in Path(data_file).open().readlines()]\n",
    "\n",
    "# data_file = \"../data/hc/valid.src\"\n",
    "# data +=[eval(x.strip(' \\n\\t\\r),\\\\')) for x in Path(data_file).open().readlines()]\n",
    "\n",
    "# print(\"data num is:\", len(data))\n",
    "\n",
    "# n = int(len(data)/48)\n",
    "# data_list =[data[i:i + n] for i in range(0, len(data), n)]\n",
    "\n",
    "# with Pool(48) as p:\n",
    "\n",
    "#     data =list(tqdm(p.imap(encoder_data, data_list), total=len(data_list)))\n",
    "\n",
    "# record += [item for subl in data for item in subl]\n",
    "\n",
    "#######################################commonsence dataset########################################\n",
    "\n",
    "# json_path = \"../data/commongen.train.jsonl\"\n",
    "# with open(json_path) as out:\n",
    "#     lines = out.readlines()\n",
    "\n",
    "#     for l in tqdm(lines):\n",
    "#         item = json.loads(l.strip())\n",
    "#         concept_set = item['concept_set']\n",
    "#         for c in item['scene']:\n",
    "#             c = c.strip()\n",
    "#             # if c.endswith('.'):\n",
    "#             #     c= c[:-1].strip()\n",
    "#             # if c[0].islower():\n",
    "#             #     continue\n",
    "#             record.append({\n",
    "#                             \"content\":\"\",\n",
    "#                             \"target\":c,\n",
    "#                             \"keywords\":concept_set})\n",
    "\n",
    "#######################################end########################################\n",
    "random.shuffle(record) \n",
    "\n",
    "print(\"total data number is:\", len(record))\n",
    "out_file_train.write(json.dumps(record[:int(len(record)*0.99)])+'\\n')\n",
    "out_file_train.flush()\n",
    "\n",
    "out_file_val.write(json.dumps(record[int(len(record)*0.01):])+'\\n')\n",
    "out_file_val.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f627624f-ea4b-439f-acfb-891a1047e915",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 32651/32651 [00:00<00:00, 277457.89it/s]\n"
     ]
    }
   ],
   "source": [
    "json_path = \"../data/commongen.train.jsonl\"\n",
    "data = []\n",
    "with open(json_path) as out:\n",
    "    lines = out.readlines()\n",
    "\n",
    "    for l in tqdm(lines):\n",
    "        item = json.loads(l.strip())\n",
    "        concept_set = item['concept_set']\n",
    "        for c in item['scene']:\n",
    "            c = c.strip()\n",
    "            if c[0].islower():\n",
    "                continue\n",
    "            data.append({\n",
    "                            \"content\":\"\",\n",
    "                            \"target\":c,\n",
    "                            \"keywords\":concept_set})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "488f5d15-965a-41f0-bbef-10dc0f29950d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15179"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f42ce34-f6e1-4f8d-97fe-6cb9f7f184eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ni hao hello\n"
     ]
    }
   ],
   "source": [
    "a =\"ni hao hello\"\n",
    "c = a.split('.')[0]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0abec379-6716-4dce-a413-ace2fcfc5980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 158344/158344 [00:00<00:00, 1023837.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "json_path = os.path.join('./data/', 'wiki_train.json')\n",
    "data = []\n",
    "with open(json_path) as out:\n",
    "\n",
    "    lines = json.load(out)\n",
    "    for item in tqdm(lines):\n",
    "        c = item[\"target\"]\n",
    "        if c.endswith('.'):\n",
    "            c= c[:-1].strip()+'.'\n",
    "        \n",
    "        data.append({\n",
    "                            \"content\":item[\"content\"],\n",
    "                            \"target\":c,\n",
    "                            \"keywords\":item[\"keywords\"]})\n",
    "        \n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26111963-ddc2-461c-ad99-abea3b1e6360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data number is: 158344\n"
     ]
    }
   ],
   "source": [
    "out_file_train = os.path.join('./data/', 'n_wiki_train.json')\n",
    "out_file_train = open(out_file_train, 'w', encoding='utf8')\n",
    "print(\"total data number is:\", len(data))\n",
    "out_file_train.write(json.dumps(data)+'\\n')\n",
    "out_file_train.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98ae5dad-c70f-4458-b9b3-918a67a1b74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data from /home2/zhanghanqing/formatted_wikipedia ...\n",
      "data num is: 100001\n",
      "total data number is: 95038\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import json\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import random\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def encoder_data(data):\n",
    "    \n",
    "    nn = ['VERB', 'NOUN'] \n",
    "    nlp = spacy.load('en_core_web_sm')# 加载预训练模型\n",
    "\n",
    "    res = []\n",
    "    \n",
    "    for line in data:\n",
    "        \n",
    "        # cut_pos = random.randint(8,25)\n",
    "        line = line.strip(' \\n\\t\\r),\\\\')\n",
    "        \n",
    "        \n",
    "        # if line.endswith('.'):\n",
    "        #     line= line[:-1].strip()\n",
    "        \n",
    "        \n",
    "        if len(line)<3:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        if line[0].islower():\n",
    "            continue\n",
    "        \n",
    "        if \"amp\" in line:\n",
    "            continue\n",
    "        \n",
    "        split_line= line.split()\n",
    "        \n",
    "        # if cut_pos>len(split_line) or len(split_line)>150 or len(split_line)<3:\n",
    "        #     continue\n",
    "        \n",
    "        # line_content  = ' '.join(split_line[:-cut_pos])\n",
    "        # target = ' '.join(split_line[-cut_pos:])\n",
    "        \n",
    "        line_content=''\n",
    "        target = line\n",
    "                \n",
    "        doc = nlp(target.lower())\n",
    "        tokens = [token for token in doc]           # 将句子切分成单词\n",
    "        pos = [token.pos_ for token in doc]         # 词性标注\n",
    "        lem = [token.lemma_ for token in doc]       # 词性还原\n",
    "        \n",
    "        sen = []\n",
    "        for t,p,l in zip(tokens, pos, lem):\n",
    "            if (p in nn) and (len(str(l))>=3) and ('-'  not in str(l)) and ('.' not in str(l)):\n",
    "                sen.append(str(l))\n",
    "                \n",
    "        sen = list(set(sen))\n",
    "        \n",
    "        if len(sen)<4:\n",
    "            continue\n",
    "            \n",
    "        if len(sen)>5:\n",
    "            \n",
    "            sen =  random.sample(sen, random.randint(4,5))\n",
    "            \n",
    "        random.shuffle(sen)\n",
    " \n",
    "        res.append({\n",
    "                    \"content\":line_content,\n",
    "                    \"target\":target,\n",
    "                    \"keywords\": '#'.join(sen)})\n",
    "        \n",
    "    return res \n",
    "\n",
    "json_path = '/home2/zhanghanqing/formatted_wikipedia'\n",
    "\n",
    "out_file_train = os.path.join('./data/', 'wiki_sen.src')\n",
    "out_file_train = open(out_file_train, 'w', encoding='utf8')\n",
    "\n",
    "#######################################wiki dataset########################################\n",
    "\n",
    "print(\"reading data from %s ...\" % json_path)\n",
    "\n",
    "filenames = []                \n",
    "filenames += glob(os.path.join(json_path,'wiki**.format'))\n",
    "data =  []\n",
    "record = []\n",
    "\n",
    "for data_file in filenames:\n",
    "    data +=[x for x in Path(data_file).open().readlines()[-50000:]]\n",
    "    \n",
    "d_20=0  #1w\n",
    "    \n",
    "filter_data = []\n",
    "\n",
    "for d in data:\n",
    "    l_d = len(d.split())\n",
    "    \n",
    "    if l_d>25 and l_d<=32 and d_20<=100000:\n",
    "        d_20 += 1\n",
    "        filter_data.append(d)\n",
    "        \n",
    "    if d_20>100000:\n",
    "        break\n",
    "             \n",
    "data = filter_data\n",
    "print(\"data num is:\", len(data))\n",
    "\n",
    "random.shuffle(data) \n",
    "data = list(filter(lambda x: \"amp\" not in x ,data))\n",
    "print(\"total data number is:\", len(data))\n",
    "\n",
    "# lists=[line +\"\\n\" for line in data]\n",
    "out_file_train.writelines(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fd8d1b4-766c-410c-9ec2-07f990135b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19409"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58db6cc5-6c7a-428b-af70-f493bb023296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Of particular note in the Queen Mary Bedroom are two chairs covered with needlework created by Albert, who was once the chairman of the Royal School of Needlework.\\n',\n",
       " 'Formerly known as the Patchwork Bedroom, the Ante Room was established by Charlotte, Countess Spencer and her sister during the Victorian period, and today forms part of a suite of state chambers.\\n',\n",
       " \"two are Grade II* listed, including the Stable Block and Gardener's House, Althorp, and the remainder have a Grade II designation, mainly garden screens, gates and gateways aside from the planting stones.\\n\",\n",
       " 'The mustard-yellow Grade II* listed Stable Block, designed by architect Roger Morris with a Palladian influence, was ordered by Charles, Fifth Earl of Sutherland in the early 1730s.\\n',\n",
       " \"Morris designed the building with a clear Tuscan architectural design, drawing upon earlier inspiration from his stables at Inigo Jones's St Paul's Church in Covent Garden.\\n\",\n",
       " \"Several rooms were built within the stable block, including hot and cold baths for riders after hunting, a veterinarian's room with medicines for horses, and what was once a smoking room.\\n\",\n",
       " 'The hall of the falconry contains 2 arcaded openings with keystones, and to the left of the hall is a fireplace with Delft tiles, dated to the 19th century.\\n',\n",
       " 'Over the centuries at various times it has been the home of the park warden, the gamekeeper, and the kennelman who looked after the hounds, hosting puppy shows in the 20th century.\\n',\n",
       " 'Both of the North and South West Lodges of Althorp date to around 1730 under Roger Morris and are both built from lias ashlar with slate roofs.\\n',\n",
       " 'The Grade II listed Dairy Cottage, to the northwest of the lake, dates to the late 18th century under Henry Holland, and was listed in December 1986.\\n']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c8b88b4-62fc-47a8-a452-122fd024d6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data from /home2/zhanghanqing/formatted_wikipedia ...\n",
      "80001 40001 10001\n",
      "data num is: 130003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:48<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the record is: 104569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:16<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data number is: 139128\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import json\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import random\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def encoder_data_target(data):\n",
    "    \n",
    "    nn = ['VERB', 'NOUN'] \n",
    "    nlp = spacy.load('en_core_web_sm')# 加载预训练模型\n",
    "    res = []\n",
    "    \n",
    "    for line in data:\n",
    "        line_content  = ' '.join(line[:-1])\n",
    "        target = line[-1]\n",
    "        \n",
    "        if len(target.split())<6:\n",
    "            continue\n",
    "                \n",
    "        doc = nlp(target.lower())\n",
    "        tokens = [token for token in doc]           # 将句子切分成单词\n",
    "        pos = [token.pos_ for token in doc]         # 词性标注\n",
    "        lem = [token.lemma_ for token in doc]       # 词性还原\n",
    "        \n",
    "        sen = []\n",
    "        for t,p,l in zip(tokens, pos, lem):\n",
    "            if (p in nn) and (len(str(l))>=3) and ('-'  not in str(l)) and ('.' not in str(l)):\n",
    "                sen.append(str(l))\n",
    "                \n",
    "        sen = list(set(sen))\n",
    "        \n",
    "        if len(sen)<3:\n",
    "            continue\n",
    "            \n",
    "        if len(sen)>5:\n",
    "            \n",
    "            sen =  random.sample(sen, 5)\n",
    "            \n",
    "        random.shuffle(sen)\n",
    " \n",
    "        res.append({\n",
    "                    \"content\":line_content,\n",
    "                    \"target\":target,\n",
    "                    \"keywords\": '#'.join(sen)})\n",
    "        \n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def encoder_data(data):\n",
    "    \n",
    "    nn = ['VERB', 'NOUN'] \n",
    "    nlp = spacy.load('en_core_web_sm')# 加载预训练模型\n",
    "\n",
    "    res = []\n",
    "    \n",
    "    for line in data:\n",
    "        \n",
    "        cut_pos = random.randint(8,25)\n",
    "        line = line.strip(' \\n\\t\\r),\\\\')\n",
    "        \n",
    "        if \"amp\" in line:\n",
    "            continue\n",
    "        \n",
    "        split_line= line.split()\n",
    "        \n",
    "        # if cut_pos>len(split_line) or len(split_line)>150 or len(split_line)<3:\n",
    "        #     continue\n",
    "        \n",
    "        \n",
    "        line_content  = ' '.join(split_line[:-cut_pos])\n",
    "        target = ' '.join(split_line[-cut_pos:])\n",
    "                \n",
    "        doc = nlp(target.lower())\n",
    "        tokens = [token for token in doc]           # 将句子切分成单词\n",
    "        pos = [token.pos_ for token in doc]         # 词性标注\n",
    "        lem = [token.lemma_ for token in doc]       # 词性还原\n",
    "        \n",
    "        sen = []\n",
    "        for t,p,l in zip(tokens, pos, lem):\n",
    "            if (p in nn) and (len(str(l))>=3) and ('-'  not in str(l)) and ('.' not in str(l)):\n",
    "                sen.append(str(l))\n",
    "                \n",
    "        sen = list(set(sen))\n",
    "        \n",
    "        if len(sen)<3:\n",
    "            continue\n",
    "            \n",
    "        if len(sen)>5:\n",
    "            \n",
    "            sen =  random.sample(sen, random.randint(3,5))\n",
    "            \n",
    "        random.shuffle(sen)\n",
    " \n",
    "        res.append({\n",
    "                    \"content\":line_content,\n",
    "                    \"target\":target,\n",
    "                    \"keywords\": '#'.join(sen)})\n",
    "        \n",
    "    return res \n",
    "\n",
    "json_path = '/home2/zhanghanqing/formatted_wikipedia'\n",
    "\n",
    "out_file_train = os.path.join('./data/', 'wiki_train.json')\n",
    "out_file_train = open(out_file_train, 'w', encoding='utf8')\n",
    "\n",
    "\n",
    "out_file_val = os.path.join('./data/', 'wiki_val.json')\n",
    "out_file_val = open(out_file_val, 'w', encoding='utf8')\n",
    "\n",
    "\n",
    "#######################################wiki dataset########################################\n",
    "\n",
    "print(\"reading data from %s ...\" % json_path)\n",
    "\n",
    "filenames = []                \n",
    "filenames += glob(os.path.join(json_path,'wiki**.format'))\n",
    "\n",
    "data =  []\n",
    "\n",
    "for data_file in filenames:\n",
    "    data +=[x for x in Path(data_file).open().readlines()[-3000000:]]\n",
    "random.shuffle(data)\n",
    "\n",
    "d_0_50 = 0  #5w\n",
    "d_50_90 = 0 #3w\n",
    "d_90=0  #1w\n",
    "\n",
    "    \n",
    "filter_data = []\n",
    "\n",
    "for d in data:\n",
    "    l_d = len(d.split())\n",
    "    \n",
    "    if d_0_50>80000 and d_50_90>40000 and d_90>10000:\n",
    "        break\n",
    "    \n",
    "    if l_d<50 and d_0_50<=80000:\n",
    "        d_0_50 +=1\n",
    "        filter_data.append(d)\n",
    "            \n",
    "        \n",
    "    elif l_d>=50 and l_d<90 and d_50_90<=40000:\n",
    "        d_50_90 +=1\n",
    "        filter_data.append(d)\n",
    "        \n",
    "    elif l_d>=90 and d_90<=10000:\n",
    "        d_90+=1\n",
    "        filter_data.append(d)\n",
    "             \n",
    "print(d_0_50,d_50_90, d_90)\n",
    "        \n",
    "data = filter_data\n",
    "print(\"data num is:\", len(data))\n",
    "\n",
    "n = int(len(data)/48)\n",
    "data_list =[data[i:i + n] for i in range(0, len(data), n)]\n",
    "\n",
    "with Pool(48) as p:\n",
    "\n",
    "    data =list(tqdm(p.imap(encoder_data, data_list), total=len(data_list)))\n",
    "\n",
    "record = [item for subl in data for item in subl]\n",
    "\n",
    "print(\"the record is:\",len(record))\n",
    "\n",
    "#######################################roc dataset########################################\n",
    "roc = pd.read_csv(\"../data/roc/roc.csv\")\n",
    "data = []\n",
    "for row in roc.itertuples():\n",
    "    # if random.randint(1,5)%5==1:\n",
    "    cut_pos = random.randint(3,5)\n",
    "    # else:\n",
    "    #     cut_pos = 5\n",
    "    context = [row[i+3] for i in range(cut_pos)]\n",
    "    data.append(context)\n",
    "    \n",
    "random.shuffle(data)\n",
    "\n",
    "\n",
    "n = int(len(data)/48)\n",
    "data_list =[data[i:i + n] for i in range(0, len(data), n)]\n",
    "\n",
    "with Pool(48) as p:\n",
    "\n",
    "    data =list(tqdm(p.imap(encoder_data_target, data_list), total=len(data_list)))\n",
    "\n",
    "record += [item for subl in data for item in subl]\n",
    "\n",
    "#######################################hc dataset########################################\n",
    "# data_file = \"../data/hc/train.src\"\n",
    "# data =[eval(x.strip(' \\n\\t\\r),\\\\')) for x in Path(data_file).open().readlines()]\n",
    "\n",
    "# data_file = \"../data/hc/valid.src\"\n",
    "# data +=[eval(x.strip(' \\n\\t\\r),\\\\')) for x in Path(data_file).open().readlines()]\n",
    "\n",
    "# print(\"data num is:\", len(data))\n",
    "\n",
    "# n = int(len(data)/48)\n",
    "# data_list =[data[i:i + n] for i in range(0, len(data), n)]\n",
    "\n",
    "# with Pool(48) as p:\n",
    "\n",
    "#     data =list(tqdm(p.imap(encoder_data, data_list), total=len(data_list)))\n",
    "\n",
    "# record += [item for subl in data for item in subl]\n",
    "\n",
    "#######################################commonsence dataset########################################\n",
    "\n",
    "\n",
    "# json_path = \"../data/commongen.train.jsonl\"\n",
    "# with open(json_path) as out:\n",
    "#     lines = out.readlines()\n",
    "\n",
    "#     for l in tqdm(lines):\n",
    "#         item = json.loads(l.strip())\n",
    "#         concept_set = item['concept_set']\n",
    "#         for c in item['scene']:\n",
    "#             c = c.strip()\n",
    "#         record.append({\n",
    "#                         \"content\":\"\",\n",
    "#                         \"target\":c,\n",
    "#                         \"keywords\":concept_set})\n",
    "\n",
    "#######################################end########################################\n",
    "\n",
    "\n",
    "random.shuffle(record)\n",
    "\n",
    "print(\"total data number is:\", len(record))\n",
    "out_file_train.write(json.dumps(record[:int(len(record)*0.95)])+'\\n')\n",
    "out_file_train.flush()\n",
    "\n",
    "out_file_val.write(json.dumps(record[int(len(record)*0.95):])+'\\n')\n",
    "out_file_val.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28039dae-378a-4995-8975-eecea2e27f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py39]",
   "language": "python",
   "name": "conda-env-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
