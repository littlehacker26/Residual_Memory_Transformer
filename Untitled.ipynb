{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c27bbae-fdcf-4e31-ba4c-5e39548022f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import transformers\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    BertTokenizer,\n",
    "    GPT2Tokenizer\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
    "from transformers import GPT2LMHeadModel, AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "model_name_or_path = \"/home2/zhanghanqing/pretrained_model/gpt2/large\"\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "            model_name_or_path)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2733f4a0-05e0-41b6-8e82-17aa461d0fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-24_16:20:36.804980\n"
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "\n",
    "t = str(datetime.datetime.now()).replace(\" \",\"_\")\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33325704-aa65-43fe-92ce-c57e097e5155",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only assign an iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a[:\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only assign an iterable"
     ]
    }
   ],
   "source": [
    "a[:2]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3848fbd-38b2-42c1-8909-1772d862b339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [], 'attention_mask': []}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7a23761-f38e-45b4-9d5d-ea04a0896f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [20342, 11, 23748, 995], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"hey, hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4104c5e3-77d1-44b8-bc3c-99e0ff574211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [23748, 995], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\" hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "7ac9e34a-0b4f-47ae-a4db-18289f1c9198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_metric import cal_ppl_bygpt2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "text = [\"I love you\", \"love pretrain hello\"]\n",
    "\n",
    "ppl = cal_ppl_bygpt2(tokenizer, model, 20, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efad93ea-3c70-4c73-a1b6-c9c08fec07af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [464], 'attention_mask': [1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"The\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c7204bc5-909f-4313-bf68-4eaf66c084fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0.dev20221024+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "708a8258-75a3-4620-86ac-d4a5683b5b41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhanghanqing/anaconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import copy\n",
    "from typing import Optional, Any, Union, Callable\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import MultiheadAttention\n",
    "from torch.nn import Dropout\n",
    "from torch.nn import Linear\n",
    "from torch.nn import LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b8572ad-e428-447b-91ad-9fbfd9cf7d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhanghanqing/anaconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from adapters.untitled  import Transformer_Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2f45e8c-14d2-4e09-b7d8-e163fa29dfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c21129b-387c-40c0-8e99-4caff433f34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "decoder_layer = Transformer_Decoder(d_model=512, nhead=8, batch_first=True)\n",
    "memory = torch.rand(32, 10, 512)\n",
    "tgt = torch.rand(32, 20, 512)\n",
    "out = decoder_layer(tgt, tgt, memory)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f6217-8767-4a5c-9e78-68af520afdf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9a04fc76-6c05-466a-96c8-9ee9ae5135c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 15496,    11,   616,  3290,   318,   257,  1310,  1643,   286,\n",
      "           257,  2356,   287,   262,   840,    11,   475,   339,   338,   257],\n",
      "        [ 8206,   290,  2456,  2291,    25, 16569,    11,  9280,    11,  1620,\n",
      "            11,  3800,    11,  5806,    13,  7253,  5270,    25,   352,    12,\n",
      "            17,  2745,    13,   198,   198,    16,    12,    17,  2745,    13]])\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is a little bit of a pain in the ass, but he's a\n",
      "Text and words include: costume, dance, perform, stage, wear. Start generation: 1-2 weeks.\n",
      "\n",
      "1-2 weeks.\n"
     ]
    }
   ],
   "source": [
    "tokenizer.padding_side = \"left\" \n",
    "tokenizer.pad_token = tokenizer.eos_token # to avoid an error\n",
    "\n",
    "sentences = [\"Hello, my dog is a little\",\n",
    "            \"Text and words include: costume, dance, perform, stage, wear. Start generation:\"\n",
    "            ]\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "output_sequences = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    num_beams=3,\n",
    "    do_sample=False, \n",
    "    max_length=30 # disable sampling to test if batching affects output\n",
    ")\n",
    "print(output_sequences)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    print(tokenizer.decode(output_sequences[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d40d148-5be5-4729-a416-f5e561af7530",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data from /home2/zhanghanqing/formatted_wikipedia ...\n",
      "150001 30001 967\n",
      "data num is: 180002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 49/49 [01:09<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the record is: 127497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 32651/32651 [00:00<00:00, 214306.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data number is: 142676\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import json\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import random\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def encoder_data_target(data):\n",
    "    \n",
    "    nn = ['VERB', 'NOUN'] \n",
    "    nlp = spacy.load('en_core_web_sm')# 加载预训练模型\n",
    "    res = []\n",
    "    \n",
    "    for line in data:\n",
    "        line_content  = ' '.join(line[:-1])\n",
    "        target = line[-1]\n",
    "        \n",
    "        if len(target.split())<6:\n",
    "            continue\n",
    "                \n",
    "        doc = nlp(target.lower())\n",
    "        tokens = [token for token in doc]           # 将句子切分成单词\n",
    "        pos = [token.pos_ for token in doc]         # 词性标注\n",
    "        lem = [token.lemma_ for token in doc]       # 词性还原\n",
    "        \n",
    "        sen = []\n",
    "        for t,p,l in zip(tokens, pos, lem):\n",
    "            if (p in nn) and (len(str(l))>=3) and ('-'  not in str(l)) and ('.' not in str(l)):\n",
    "                sen.append(str(l))\n",
    "                \n",
    "        sen = list(set(sen))\n",
    "        \n",
    "        if len(sen)<3:\n",
    "            continue\n",
    "            \n",
    "        if len(sen)>5:\n",
    "            \n",
    "            sen =  random.sample(sen, 5)\n",
    "            \n",
    "        random.shuffle(sen)\n",
    " \n",
    "        res.append({\n",
    "                    \"content\":line_content,\n",
    "                    \"target\":target,\n",
    "                    \"keywords\": '#'.join(sen)})\n",
    "        \n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def encoder_data(data):\n",
    "    \n",
    "    nn = ['VERB', 'NOUN'] \n",
    "    nlp = spacy.load('en_core_web_sm')# 加载预训练模型\n",
    "\n",
    "    res = []\n",
    "    \n",
    "    for line in data:\n",
    "        \n",
    "        cut_pos = random.randint(8,16)\n",
    "        line = line.strip(' \\n\\t\\r),\\\\')\n",
    "        \n",
    "        if \"amp\" in line:\n",
    "            continue\n",
    "        \n",
    "        split_line= line.split()\n",
    "        \n",
    "        if cut_pos>len(split_line) or len(split_line)>150 or len(split_line)<3:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        line_content  = ' '.join(split_line[:-cut_pos])\n",
    "        target = ' '.join(split_line[-cut_pos:])\n",
    "                \n",
    "        doc = nlp(target.lower())\n",
    "        tokens = [token for token in doc]           # 将句子切分成单词\n",
    "        pos = [token.pos_ for token in doc]         # 词性标注\n",
    "        lem = [token.lemma_ for token in doc]       # 词性还原\n",
    "        \n",
    "        sen = []\n",
    "        for t,p,l in zip(tokens, pos, lem):\n",
    "            if (p in nn) and (len(str(l))>=3) and ('-'  not in str(l)) and ('.' not in str(l)):\n",
    "                sen.append(str(l))\n",
    "                \n",
    "        sen = list(set(sen))\n",
    "        \n",
    "        if len(sen)<3:\n",
    "            continue\n",
    "            \n",
    "        if len(sen)>5:\n",
    "            \n",
    "            sen =  random.sample(sen, random.randint(3,5))\n",
    "            \n",
    "        random.shuffle(sen)\n",
    " \n",
    "        res.append({\n",
    "                    \"content\":line_content,\n",
    "                    \"target\":target,\n",
    "                    \"keywords\": '#'.join(sen)})\n",
    "        \n",
    "    return res \n",
    "\n",
    "json_path = '/home2/zhanghanqing/formatted_wikipedia'\n",
    "\n",
    "out_file_train = os.path.join('./data/', 'wiki_train.json')\n",
    "out_file_train = open(out_file_train, 'w', encoding='utf8')\n",
    "\n",
    "\n",
    "out_file_val = os.path.join('./data/', 'wiki_val.json')\n",
    "out_file_val = open(out_file_val, 'w', encoding='utf8')\n",
    "\n",
    "\n",
    "#######################################wiki dataset########################################\n",
    "\n",
    "print(\"reading data from %s ...\" % json_path)\n",
    "\n",
    "filenames = []                \n",
    "filenames += glob(os.path.join(json_path,'wiki**.format'))\n",
    "\n",
    "data =  []\n",
    "\n",
    "for data_file in filenames:\n",
    "    data +=[x for x in Path(data_file).open().readlines()[-200000:]]\n",
    "    \n",
    "d_0_50 = 0  #6w\n",
    "d_50_90 = 0 #3w\n",
    "d_90=0  #1w\n",
    "    \n",
    "filter_data = []  \n",
    "\n",
    "for d in data:\n",
    "    l_d = len(d.split())\n",
    "    \n",
    "    if d_0_50>150000 and d_50_90>30000 and d_90>10000:\n",
    "        break\n",
    "    \n",
    "    if l_d<50 and d_0_50<=150000:\n",
    "        d_0_50 +=1\n",
    "        filter_data.append(d)\n",
    "        \n",
    "    elif l_d>=50 and l_d<90 and d_50_90<=30000:\n",
    "        d_50_90 +=1\n",
    "        filter_data.append(d)\n",
    "        \n",
    "    elif l_d>=90 and d_90<=10000:\n",
    "        d_90+=1\n",
    "        # filter_data.append(d)\n",
    "             \n",
    "print(d_0_50,d_50_90, d_90)\n",
    "        \n",
    "data = filter_data\n",
    "print(\"data num is:\", len(data))\n",
    "\n",
    "n = int(len(data)/48)\n",
    "data_list =[data[i:i + n] for i in range(0, len(data), n)]\n",
    "\n",
    "with Pool(48) as p:\n",
    "\n",
    "    data =list(tqdm(p.imap(encoder_data, data_list), total=len(data_list)))\n",
    "\n",
    "record = [item for subl in data for item in subl]\n",
    "\n",
    "print(\"the record is:\",len(record))\n",
    "\n",
    "#######################################roc dataset########################################\n",
    "# roc = pd.read_csv(\"../data/roc/roc.csv\")\n",
    "# data = []\n",
    "# for row in roc.itertuples():\n",
    "#     # if random.randint(1,5)%5==1:\n",
    "#     cut_pos = random.randint(3,5)\n",
    "#     # else:\n",
    "#     #     cut_pos = 5\n",
    "#     context = [row[i+3] for i in range(cut_pos)]\n",
    "#     data.append(context)\n",
    "    \n",
    "# random.shuffle(data)\n",
    "\n",
    "\n",
    "# n = int(len(data)/48)\n",
    "# data_list =[data[i:i + n] for i in range(0, len(data), n)]\n",
    "\n",
    "# with Pool(48) as p:\n",
    "\n",
    "#     data =list(tqdm(p.imap(encoder_data_target, data_list), total=len(data_list)))\n",
    "\n",
    "# record += [item for subl in data for item in subl]\n",
    "\n",
    "#######################################hc dataset########################################\n",
    "# data_file = \"../data/hc/train.src\"\n",
    "# data =[eval(x.strip(' \\n\\t\\r),\\\\')) for x in Path(data_file).open().readlines()]\n",
    "\n",
    "# data_file = \"../data/hc/valid.src\"\n",
    "# data +=[eval(x.strip(' \\n\\t\\r),\\\\')) for x in Path(data_file).open().readlines()]\n",
    "\n",
    "# print(\"data num is:\", len(data))\n",
    "\n",
    "# n = int(len(data)/48)\n",
    "# data_list =[data[i:i + n] for i in range(0, len(data), n)]\n",
    "\n",
    "# with Pool(48) as p:\n",
    "\n",
    "#     data =list(tqdm(p.imap(encoder_data, data_list), total=len(data_list)))\n",
    "\n",
    "# record += [item for subl in data for item in subl]\n",
    "\n",
    "#######################################commonsence dataset########################################\n",
    "\n",
    "\n",
    "json_path = \"../data/commongen.train.jsonl\"\n",
    "with open(json_path) as out:\n",
    "    lines = out.readlines()\n",
    "\n",
    "    for l in tqdm(lines):\n",
    "        item = json.loads(l.strip())\n",
    "        concept_set = item['concept_set']\n",
    "        for c in item['scene']:\n",
    "            c = c.strip()\n",
    "            if c[0].islower():\n",
    "                continue\n",
    "            record.append({\n",
    "                            \"content\":\"\",\n",
    "                            \"target\":c,\n",
    "                            \"keywords\":concept_set})\n",
    "\n",
    "#######################################end########################################\n",
    "\n",
    "\n",
    "random.shuffle(record)\n",
    "\n",
    "print(\"total data number is:\", len(record))\n",
    "out_file_train.write(json.dumps(record[:int(len(record)*0.95)])+'\\n')\n",
    "out_file_train.flush()\n",
    "\n",
    "out_file_val.write(json.dumps(record[int(len(record)*0.95):])+'\\n')\n",
    "out_file_val.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f627624f-ea4b-439f-acfb-891a1047e915",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 32651/32651 [00:00<00:00, 277457.89it/s]\n"
     ]
    }
   ],
   "source": [
    "json_path = \"../data/commongen.train.jsonl\"\n",
    "data = []\n",
    "with open(json_path) as out:\n",
    "    lines = out.readlines()\n",
    "\n",
    "    for l in tqdm(lines):\n",
    "        item = json.loads(l.strip())\n",
    "        concept_set = item['concept_set']\n",
    "        for c in item['scene']:\n",
    "            c = c.strip()\n",
    "            if c[0].islower():\n",
    "                continue\n",
    "            data.append({\n",
    "                            \"content\":\"\",\n",
    "                            \"target\":c,\n",
    "                            \"keywords\":concept_set})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "488f5d15-965a-41f0-bbef-10dc0f29950d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15179"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f42ce34-f6e1-4f8d-97fe-6cb9f7f184eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': '',\n",
       "  'target': 'Skier skis down the mountain',\n",
       "  'keywords': 'mountain#ski#skier'},\n",
       " {'content': '',\n",
       "  'target': 'A skier is skiing down a mountain.',\n",
       "  'keywords': 'mountain#ski#skier'},\n",
       " {'content': '',\n",
       "  'target': 'Three skiers are skiing on a snowy mountain.',\n",
       "  'keywords': 'mountain#ski#skier'},\n",
       " {'content': '',\n",
       "  'target': 'The dog is wagging his tail.',\n",
       "  'keywords': 'dog#tail#wag'},\n",
       " {'content': '',\n",
       "  'target': 'A dog wags his tail at the boy.',\n",
       "  'keywords': 'dog#tail#wag'},\n",
       " {'content': '',\n",
       "  'target': 'A horse is eating hay.',\n",
       "  'keywords': 'eat#hay#horse'},\n",
       " {'content': '',\n",
       "  'target': 'The horses are eating hay.',\n",
       "  'keywords': 'eat#hay#horse'},\n",
       " {'content': '',\n",
       "  'target': 'A horse eats hay in the barn',\n",
       "  'keywords': 'eat#hay#horse'},\n",
       " {'content': '',\n",
       "  'target': 'A dog laying on a rug.',\n",
       "  'keywords': 'dog#lay#rug'},\n",
       " {'content': '',\n",
       "  'target': 'The dogs laid down on the rug',\n",
       "  'keywords': 'dog#lay#rug'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0abec379-6716-4dce-a413-ace2fcfc5980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137373"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "98ae5dad-c70f-4458-b9b3-918a67a1b74a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <spacy_ke.yake.Yake object at 0x7f530a7ba4c0> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [92]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;66;03m# 加载预训练模型\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# nlp.add_pipe(\"yake\")\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_pipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mYake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# default\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlemmatize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# default\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_selection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mngram\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# default, use \"chunk\" for noun phrase selection\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m nlp\u001b[38;5;241m.\u001b[39madd_pipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myake\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(target)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/spacy/language.py:779\u001b[0m, in \u001b[0;36mLanguage.add_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    777\u001b[0m     bad_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrepr\u001b[39m(factory_name)\n\u001b[1;32m    778\u001b[0m     err \u001b[38;5;241m=\u001b[39m Errors\u001b[38;5;241m.\u001b[39mE966\u001b[38;5;241m.\u001b[39mformat(component\u001b[38;5;241m=\u001b[39mbad_val, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m--> 779\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err)\n\u001b[1;32m    780\u001b[0m name \u001b[38;5;241m=\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m factory_name\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent_names:\n",
      "\u001b[0;31mValueError\u001b[0m: [E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <spacy_ke.yake.Yake object at 0x7f530a7ba4c0> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import random\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import spacy_ke\n",
    "from spacy_ke import Yake\n",
    "\n",
    "\n",
    "target = \"The metal content of ore was increased through the process to make transportation less expensive.\"\n",
    "nlp = spacy.load('en_core_web_sm')# 加载预训练模型\n",
    "\n",
    "\n",
    "# nlp.add_pipe(\"yake\")\n",
    "\n",
    "nlp.add_pipe(\n",
    "    Yake(\n",
    "        nlp,\n",
    "        window=1,  # default\n",
    "        lemmatize=False,  # default\n",
    "        candidate_selection=\"ngram\"  # default, use \"chunk\" for noun phrase selection\n",
    "    )\n",
    ")\n",
    "nlp.add_pipe(\"yake\")\n",
    "\n",
    "doc = nlp(target)\n",
    "words = []\n",
    "\n",
    "for keyword, score in doc._.extract_keywords(n=3):\n",
    "    print(keyword, \"-\", score)\n",
    "    \n",
    "# for phrase in doc._.phrases:\n",
    "#     # print(phrase.rank, phrase.count)\n",
    "#     print(phrase.chunks[0])\n",
    "#     words.append(phrase.chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "840dd2e7-564e-41ca-b7f1-021a549803cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(random.randint(9,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65bac06f-a4cf-4619-bc48-b45f5a6767a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the cake, top with sugar free powder sprinkles.\n"
     ]
    }
   ],
   "source": [
    "a = \"For the cake, top with sugar-free powder sprinkles.\"\n",
    "b = a.replace(\"-\",' ')\n",
    "print(b)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "def tokenize(dict):\n",
    "    for key in dict:\n",
    "        new_sentence_list = []\n",
    "        for sentence in dict[key]:\n",
    "            a = ''\n",
    "            for token in nlp(sentence):\n",
    "                a += token.text\n",
    "                a += ' '\n",
    "            new_sentence_list.append(a.rstrip())\n",
    "        dict[key] = new_sentence_list\n",
    "\n",
    "    return dict\n",
    "\n",
    "def get_coverage_score(gt_concepts, pred):\n",
    "    covs = []\n",
    "    total_cs, match_cs = 0, 0\n",
    "    for cs, p in zip(gt_concepts, pred):\n",
    "        p = p.lower()\n",
    "        if p.endswith('.'):\n",
    "            p = p[:-1]\n",
    "            p = p.strip()\n",
    "        cs = set(cs)\n",
    "        lemmas = set()\n",
    "        for token in nlp(p):\n",
    "            lemmas.add(token.lemma_)\n",
    "        match_cs += len(lemmas&cs)\n",
    "        total_cs += len(cs)\n",
    "        cov = len(lemmas&cs)/len(cs)\n",
    "        covs.append(cov)\n",
    "    return 100 * sum(covs) / len(covs), 100 * match_cs / total_cs\n",
    "\n",
    "\n",
    "\n",
    "def evaluator_coverage(res):\n",
    "    \n",
    "    pred = tokenize(res)\n",
    "    gt_concepts = [r.split('#') for r in list(pred.keys())]\n",
    "    \n",
    "    predictions = [v[0] for k,v in pred.items()]\n",
    "    \n",
    "    print(gt_concepts)\n",
    "    print(predictions)\n",
    "\n",
    "    score = get_coverage_score(gt_concepts, predictions)\n",
    "    print(score)\n",
    "    \n",
    "    return mean(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81612647-f0f0-4a9b-a6a9-0340d6a1a9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sugar', 'free', 'powder']]\n",
      "['For the cake , top with sugar - free powder sprinkle .']\n",
      "(100.0, 100.0)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m res\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msugar#free#powder\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor the cake, top with sugar-free powder sprinkle.\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m----> 3\u001b[0m \u001b[43mevaluator_coverage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36mevaluator_coverage\u001b[0;34m(res)\u001b[0m\n\u001b[1;32m     51\u001b[0m score \u001b[38;5;241m=\u001b[39m get_coverage_score(gt_concepts, predictions)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(score)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmean\u001b[49m(score)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean' is not defined"
     ]
    }
   ],
   "source": [
    "res={\"sugar#free#powder\": [\"For the cake, top with sugar-free powder sprinkle.\"]}\n",
    "\n",
    "evaluator_coverage(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1ed53b5-c819-490f-a390-866f405fc5a4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F',\n",
       " 'o',\n",
       " 'r',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " '',\n",
       " 'c',\n",
       " 'a',\n",
       " 'k',\n",
       " 'e',\n",
       " ',',\n",
       " '',\n",
       " 't',\n",
       " 'o',\n",
       " 'p',\n",
       " '',\n",
       " 'w',\n",
       " 'i',\n",
       " 't',\n",
       " 'h',\n",
       " '',\n",
       " 's',\n",
       " 'u',\n",
       " 'g',\n",
       " 'a',\n",
       " 'r',\n",
       " '-',\n",
       " 'f',\n",
       " 'r',\n",
       " 'e',\n",
       " 'e',\n",
       " '',\n",
       " 'p',\n",
       " 'o',\n",
       " 'w',\n",
       " 'd',\n",
       " 'e',\n",
       " 'r',\n",
       " '',\n",
       " 's',\n",
       " 'p',\n",
       " 'r',\n",
       " 'i',\n",
       " 'n',\n",
       " 'k',\n",
       " 'l',\n",
       " 'e',\n",
       " 's',\n",
       " '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"sugar#free#powder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9e593f4-65eb-4c82-8b96-7bcf576f82bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = {\n",
    "\"1\":[\"Today is a nice days\"],\n",
    "\"2\":[\"I like  huggingface, it is easy to use\"],\n",
    "\"3\":[\"big model is changing the worlds\"]\n",
    "}\n",
    "\n",
    "gts = {\n",
    "\"1\":[\"Today is a nice day\"],\n",
    "\"2\":[\"I like huggingface, it is easy to use\"],\n",
    "\"3\":[\"big model is changing the worlds\"]\n",
    "}\n",
    "\n",
    "# ref = [v for k, v in reference.items()]\n",
    "\n",
    "# cand = [v for k, v in text.items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "885edf37-b05e-4fba-85fb-ece4b40bb309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': ['today is a nice day'], '2': ['i like huggingface it is easy to use'], '3': ['big model is changing the worlds']} {'1': ['today is a nice days'], '2': ['i like huggingface it is easy to use'], '3': ['big model is changing the world']}\n"
     ]
    }
   ],
   "source": [
    "from speaksee import evaluation\n",
    "\n",
    "def tokenize(dict):\n",
    "    for key in dict:\n",
    "        new_sentence_list = []\n",
    "        for sentence in dict[key]:\n",
    "            a = ''\n",
    "            for token in nlp(sentence):\n",
    "                a += token.text\n",
    "                a += ' '\n",
    "            new_sentence_list.append(a.rstrip())\n",
    "        dict[key] = new_sentence_list\n",
    "\n",
    "    return dict\n",
    "\n",
    "# gts = tokenize(gts)\n",
    "# gen = tokenize(gen)\n",
    "\n",
    "gts = evaluation.PTBTokenizer.tokenize(gts)\n",
    "gen = evaluation.PTBTokenizer.tokenize(gen)\n",
    "\n",
    "\n",
    "print(gts,gen)\n",
    "# val_bleu, _ = evaluation.Bleu(n=4).compute_score(gts, gen)\n",
    "# method = ['Blue_1', 'Bleu_2', 'Bleu_3', 'Bleu_4']\n",
    "# metric_dict = {}\n",
    "# for metric, score in zip(method, val_bleu):\n",
    "#     metric_dict[metric] = {'entire': score * 100}\n",
    "#     print('%s %.2f' % (metric, score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf07bc0c-3c9e-402a-bf0d-449519e59481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['today is a nice day'] ['today is a nice days']\n",
      "['i like huggingface it is easy to use'] ['i like huggingface it is easy to use']\n",
      "['big model is changing the worlds'] ['big model is changing the world']\n",
      "NIST 3.85\n",
      "BLEU 85.32\n"
     ]
    }
   ],
   "source": [
    "from eval_metric import BLEUScore, NISTScore\n",
    "\n",
    "bleu = BLEUScore()\n",
    "nist = NISTScore()\n",
    "\n",
    "for sents_ref, sent_sys in zip(gts.values(), gen.values()):\n",
    "    print(sents_ref, sent_sys)\n",
    "    \n",
    "    bleu.append(sent_sys[0], sents_ref)\n",
    "    nist.append(sent_sys[0], sents_ref)\n",
    "    \n",
    "print(\"NIST %.2f\" % (nist.score()))\n",
    "print(\"BLEU %.2f\" % (bleu.score() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "fa67331a-7bc0-41bb-91f3-faefb90b26e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Today', 'is', 'a', 'nice', 'day']]\n",
      "[['I', 'like', 'huggingface,', 'it', 'is', 'easy', 'to', 'use']]\n",
      "[['big', 'model', 'is', 'changing', 'the', 'world']]\n",
      "[0.7745966692414834, 0.6976750600527641, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bigram': 0.8240905764314158,\n",
       " 'trigram': 0.7785575236143242,\n",
       " 'quagram': 0.7311587009123848}"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fast_bleu import BLEU, SelfBLEU\n",
    "import numpy as np\n",
    "\n",
    "evaluator_bleu(text, reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "305be2c2-1146-4c99-9b7d-2814cf38bf73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['big', 'model', 'is', 'changing', 'the', 'world']"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference[\"3\"][0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1343487-8351-4155-a73b-0b7f7bf65394",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,\"mask\",\"mask\",5,6,7,8,\"mask\",10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b4cb2f76-65b7-43c8-8df0-32d891a2b007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1, 2]\n",
      "[1, 2, 3]\n",
      "[1, 2, 3, 'mask']\n",
      "[1, 2, 3, 'mask']\n",
      "[1, 2, 3, 'mask', 5]\n",
      "[1, 2, 3, 'mask', 5, 6]\n",
      "[1, 2, 3, 'mask', 5, 6, 7]\n",
      "[1, 2, 3, 'mask', 5, 6, 7, 8]\n",
      "[1, 2, 3, 'mask', 5, 6, 7, 8, 'mask']\n",
      "[1, 2, 3, 'mask', 5, 6, 7, 8, 'mask', 10]\n"
     ]
    }
   ],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c16fb73f-9c97-4e65-8ea4-99e6eb15d13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 'mask', 5, 6, 7, 8, 'mask', 10]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d5ad63eb-f138-4a44-a1a1-bdfdb838d794",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (449295825.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [81]\u001b[0;36m\u001b[0m\n\u001b[0;31m    a 6\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "a 6\n",
    "if a%6 < 6:\n",
    "    print(\"hello\")\n",
    "elif a %6==1:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9055129d-d955-43cf-8226-dd0b6c07a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_set_input_ids =[1,2,3,4,5,6,7,7,9,0]\n",
    "length = len(concept_set_input_ids)\n",
    "sentinal_count = max(int(length * 0.2), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4d50ebcc-733a-4c8b-8bca-c3947e34a6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentinal_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ea03acdf-f2cd-4686-b535-9559ee754a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1026dbfe-1b87-4ece-9560-1fb1822736e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py39]",
   "language": "python",
   "name": "conda-env-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
