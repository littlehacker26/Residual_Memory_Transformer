{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c27bbae-fdcf-4e31-ba4c-5e39548022f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import transformers\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    BertTokenizer,\n",
    "    GPT2Tokenizer\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
    "from transformers import GPT2LMHeadModel, AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "model_name_or_path = \"/home2/zhanghanqing/pretrained_model/gpt2/large\"\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "            model_name_or_path)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7a23761-f38e-45b4-9d5d-ea04a0896f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [], 'attention_mask': []}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ''\n",
    "tokenizer(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae6f6217-8767-4a5c-9e78-68af520afdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a3\n"
     ]
    }
   ],
   "source": [
    "print(f\"a{len([1,3,4])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9a04fc76-6c05-466a-96c8-9ee9ae5135c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 15496,    11,   616,  3290,   318,   257,  1310,  1643,   286,\n",
      "           257,  2356,   287,   262,   840,    11,   475,   339,   338,   257],\n",
      "        [ 8206,   290,  2456,  2291,    25, 16569,    11,  9280,    11,  1620,\n",
      "            11,  3800,    11,  5806,    13,  7253,  5270,    25,   352,    12,\n",
      "            17,  2745,    13,   198,   198,    16,    12,    17,  2745,    13]])\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is a little bit of a pain in the ass, but he's a\n",
      "Text and words include: costume, dance, perform, stage, wear. Start generation: 1-2 weeks.\n",
      "\n",
      "1-2 weeks.\n"
     ]
    }
   ],
   "source": [
    "tokenizer.padding_side = \"left\" \n",
    "tokenizer.pad_token = tokenizer.eos_token # to avoid an error\n",
    "\n",
    "sentences = [\"Hello, my dog is a little\",\n",
    "            \"Text and words include: costume, dance, perform, stage, wear. Start generation:\"\n",
    "            ]\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "output_sequences = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    num_beams=3,\n",
    "    do_sample=False, \n",
    "    max_length=30 # disable sampling to test if batching affects output\n",
    ")\n",
    "print(output_sequences)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    print(tokenizer.decode(output_sequences[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d40d148-5be5-4729-a416-f5e561af7530",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data from /home2/zhanghanqing/formatted_wikipedia ...\n",
      "20001 100001 0\n",
      "data num is: 120002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:42<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the record is: 98516\n",
      "total data number is: 98516\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import json\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import random\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def encoder_data_target(data):\n",
    "    \n",
    "    nn = ['VERB', 'NOUN'] \n",
    "    nlp = spacy.load('en_core_web_sm')# 加载预训练模型\n",
    "    res = []\n",
    "    \n",
    "    for line in data:\n",
    "        line_content  = ' '.join(line[:-1])\n",
    "        target = line[-1]\n",
    "        \n",
    "        if len(target.split())<6:\n",
    "            continue\n",
    "                \n",
    "        doc = nlp(target.lower())\n",
    "        tokens = [token for token in doc]           # 将句子切分成单词\n",
    "        pos = [token.pos_ for token in doc]         # 词性标注\n",
    "        lem = [token.lemma_ for token in doc]       # 词性还原\n",
    "        \n",
    "        sen = []\n",
    "        for t,p,l in zip(tokens, pos, lem):\n",
    "            if (p in nn) and (len(str(l))>=3) and ('-'  not in str(l)) and ('.' not in str(l)):\n",
    "                sen.append(str(l))\n",
    "                \n",
    "        sen = list(set(sen))\n",
    "        \n",
    "        if len(sen)<4:\n",
    "            continue\n",
    "            \n",
    "        if len(sen)>5:\n",
    "            \n",
    "            sen =  random.sample(sen, 5)\n",
    "            \n",
    "        random.shuffle(sen)\n",
    " \n",
    "        res.append({\n",
    "                    \"content\":line_content,\n",
    "                    \"target\":target,\n",
    "                    \"keywords\": '#'.join(sen)})\n",
    "        \n",
    "    return res\n",
    "\n",
    "\n",
    "def encoder_data(data):\n",
    "    \n",
    "    nn = ['VERB', 'NOUN'] \n",
    "    nlp = spacy.load('en_core_web_sm')# 加载预训练模型\n",
    "\n",
    "    res = []\n",
    "    \n",
    "    for line in data:\n",
    "        \n",
    "        # cut_pos = random.randint(8,25)\n",
    "        line = line.strip(' \\n\\t\\r),\\\\')\n",
    "        \n",
    "        \n",
    "        # if line.endswith('.'):\n",
    "        #     line= line[:-1].strip()\n",
    "        \n",
    "        \n",
    "        if len(line)<3:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        if line[0].islower():\n",
    "            continue\n",
    "        \n",
    "        if \"amp\" in line:\n",
    "            continue\n",
    "        \n",
    "        split_line= line.split()\n",
    "        \n",
    "        # if cut_pos>len(split_line) or len(split_line)>150 or len(split_line)<3:\n",
    "        #     continue\n",
    "        \n",
    "        # line_content  = ' '.join(split_line[:-cut_pos])\n",
    "        # target = ' '.join(split_line[-cut_pos:])\n",
    "        \n",
    "        line_content=''\n",
    "        target = line\n",
    "                \n",
    "        doc = nlp(target.lower())\n",
    "        tokens = [token for token in doc]           # 将句子切分成单词\n",
    "        pos = [token.pos_ for token in doc]         # 词性标注\n",
    "        lem = [token.lemma_ for token in doc]       # 词性还原\n",
    "        \n",
    "        sen = []\n",
    "        for t,p,l in zip(tokens, pos, lem):\n",
    "            if (p in nn) and (len(str(l))>=3) and ('-'  not in str(l)) and ('.' not in str(l)):\n",
    "                sen.append(str(l))\n",
    "                \n",
    "        sen = list(set(sen))\n",
    "        \n",
    "        if len(sen)<4:\n",
    "            continue\n",
    "            \n",
    "        if len(sen)>5:\n",
    "            \n",
    "            sen =  random.sample(sen, random.randint(4,5))\n",
    "            \n",
    "        random.shuffle(sen)\n",
    " \n",
    "        res.append({\n",
    "                    \"content\":line_content,\n",
    "                    \"target\":target,\n",
    "                    \"keywords\": '#'.join(sen)})\n",
    "        \n",
    "    return res \n",
    "\n",
    "json_path = '/home2/zhanghanqing/formatted_wikipedia'\n",
    "\n",
    "out_file_train = os.path.join('./data/', 'wiki_train.json')\n",
    "out_file_train = open(out_file_train, 'w', encoding='utf8')\n",
    "\n",
    "\n",
    "out_file_val = os.path.join('./data/', 'wiki_val.json')\n",
    "out_file_val = open(out_file_val, 'w', encoding='utf8')\n",
    "\n",
    "\n",
    "#######################################wiki dataset########################################\n",
    "\n",
    "print(\"reading data from %s ...\" % json_path)\n",
    "\n",
    "filenames = []                \n",
    "filenames += glob(os.path.join(json_path,'wiki**.format'))\n",
    "data =  []\n",
    "record = []\n",
    "\n",
    "for data_file in filenames:\n",
    "    data +=[x for x in Path(data_file).open().readlines()[-40000:]]\n",
    "    \n",
    "d_0_15 = 0  #6w\n",
    "d_15_20 = 0 #3w\n",
    "d_20=0  #1w\n",
    "    \n",
    "filter_data = []  \n",
    "\n",
    "for d in data:\n",
    "    l_d = len(d.split())\n",
    "    \n",
    "    if d_0_15>20000 and d_15_20>100000 and d_20>40000:\n",
    "        break\n",
    "    \n",
    "    if l_d>=10 and l_d<15  and d_0_15<=20000:\n",
    "        d_0_15 +=1\n",
    "        filter_data.append(d)\n",
    "        \n",
    "    elif l_d>=15 and l_d<=20 and d_15_20<=100000:\n",
    "        d_15_20 +=1\n",
    "        filter_data.append(d)\n",
    "        \n",
    "    # elif l_d>20 and l_d<=32 and d_20<=40000:\n",
    "    #     d_20 += 1\n",
    "    #     filter_data.append(d)\n",
    "             \n",
    "print(d_0_15,d_15_20, d_20)\n",
    "        \n",
    "data = filter_data\n",
    "print(\"data num is:\", len(data))\n",
    "\n",
    "n = int(len(data)/48)\n",
    "data_list =[data[i:i + n] for i in range(0, len(data), n)]\n",
    "\n",
    "with Pool(48) as p:\n",
    "\n",
    "    data =list(tqdm(p.imap(encoder_data, data_list), total=len(data_list)))\n",
    "\n",
    "record = [item for subl in data for item in subl]\n",
    "\n",
    "print(\"the record is:\",len(record))\n",
    "\n",
    "#######################################roc dataset########################################\n",
    "# roc = pd.read_csv(\"../data/roc/roc.csv\")\n",
    "# data = []\n",
    "# for row in roc.itertuples():\n",
    "#     # if random.randint(1,5)%5==1:\n",
    "#     cut_pos = random.randint(3,5)\n",
    "#     # else:\n",
    "#     #     cut_pos = 5\n",
    "#     context = [row[i+3] for i in range(cut_pos)]\n",
    "#     data.append(context)\n",
    "    \n",
    "# random.shuffle(data)\n",
    "\n",
    "\n",
    "# n = int(len(data)/48)\n",
    "# data_list =[data[i:i + n] for i in range(0, len(data), n)]\n",
    "\n",
    "# with Pool(48) as p:\n",
    "\n",
    "#     data =list(tqdm(p.imap(encoder_data_target, data_list), total=len(data_list)))\n",
    "\n",
    "# record += [item for subl in data for item in subl]\n",
    "\n",
    "#######################################hc dataset########################################\n",
    "# data_file = \"../data/hc/train.src\"\n",
    "# data =[eval(x.strip(' \\n\\t\\r),\\\\')) for x in Path(data_file).open().readlines()]\n",
    "\n",
    "# data_file = \"../data/hc/valid.src\"\n",
    "# data +=[eval(x.strip(' \\n\\t\\r),\\\\')) for x in Path(data_file).open().readlines()]\n",
    "\n",
    "# print(\"data num is:\", len(data))\n",
    "\n",
    "# n = int(len(data)/48)\n",
    "# data_list =[data[i:i + n] for i in range(0, len(data), n)]\n",
    "\n",
    "# with Pool(48) as p:\n",
    "\n",
    "#     data =list(tqdm(p.imap(encoder_data, data_list), total=len(data_list)))\n",
    "\n",
    "# record += [item for subl in data for item in subl]\n",
    "\n",
    "#######################################commonsence dataset########################################\n",
    "\n",
    "# json_path = \"../data/commongen.train.jsonl\"\n",
    "# with open(json_path) as out:\n",
    "#     lines = out.readlines()\n",
    "\n",
    "#     for l in tqdm(lines):\n",
    "#         item = json.loads(l.strip())\n",
    "#         concept_set = item['concept_set']\n",
    "#         for c in item['scene']:\n",
    "#             c = c.strip()\n",
    "#             # if c.endswith('.'):\n",
    "#             #     c= c[:-1].strip()\n",
    "#             # if c[0].islower():\n",
    "#             #     continue\n",
    "#             record.append({\n",
    "#                             \"content\":\"\",\n",
    "#                             \"target\":c,\n",
    "#                             \"keywords\":concept_set})\n",
    "\n",
    "#######################################end########################################\n",
    "random.shuffle(record) \n",
    "\n",
    "print(\"total data number is:\", len(record))\n",
    "out_file_train.write(json.dumps(record[:int(len(record)*0.99)])+'\\n')\n",
    "out_file_train.flush()\n",
    "\n",
    "out_file_val.write(json.dumps(record[int(len(record)*0.01):])+'\\n')\n",
    "out_file_val.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f627624f-ea4b-439f-acfb-891a1047e915",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 32651/32651 [00:00<00:00, 277457.89it/s]\n"
     ]
    }
   ],
   "source": [
    "json_path = \"../data/commongen.train.jsonl\"\n",
    "data = []\n",
    "with open(json_path) as out:\n",
    "    lines = out.readlines()\n",
    "\n",
    "    for l in tqdm(lines):\n",
    "        item = json.loads(l.strip())\n",
    "        concept_set = item['concept_set']\n",
    "        for c in item['scene']:\n",
    "            c = c.strip()\n",
    "            if c[0].islower():\n",
    "                continue\n",
    "            data.append({\n",
    "                            \"content\":\"\",\n",
    "                            \"target\":c,\n",
    "                            \"keywords\":concept_set})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "488f5d15-965a-41f0-bbef-10dc0f29950d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15179"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f42ce34-f6e1-4f8d-97fe-6cb9f7f184eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ni hao hello\n"
     ]
    }
   ],
   "source": [
    "a =\"ni hao hello\"\n",
    "c = a.split('.')[0]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0abec379-6716-4dce-a413-ace2fcfc5980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 158344/158344 [00:00<00:00, 1023837.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "json_path = os.path.join('./data/', 'wiki_train.json')\n",
    "data = []\n",
    "with open(json_path) as out:\n",
    "\n",
    "    lines = json.load(out)\n",
    "    for item in tqdm(lines):\n",
    "        c = item[\"target\"]\n",
    "        if c.endswith('.'):\n",
    "            c= c[:-1].strip()+'.'\n",
    "        \n",
    "        data.append({\n",
    "                            \"content\":item[\"content\"],\n",
    "                            \"target\":c,\n",
    "                            \"keywords\":item[\"keywords\"]})\n",
    "        \n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26111963-ddc2-461c-ad99-abea3b1e6360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data number is: 158344\n"
     ]
    }
   ],
   "source": [
    "out_file_train = os.path.join('./data/', 'n_wiki_train.json')\n",
    "out_file_train = open(out_file_train, 'w', encoding='utf8')\n",
    "print(\"total data number is:\", len(data))\n",
    "out_file_train.write(json.dumps(data)+'\\n')\n",
    "out_file_train.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "98ae5dad-c70f-4458-b9b3-918a67a1b74a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <spacy_ke.yake.Yake object at 0x7f530a7ba4c0> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [92]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;66;03m# 加载预训练模型\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# nlp.add_pipe(\"yake\")\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_pipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mYake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# default\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlemmatize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# default\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_selection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mngram\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# default, use \"chunk\" for noun phrase selection\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m nlp\u001b[38;5;241m.\u001b[39madd_pipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myake\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(target)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/spacy/language.py:779\u001b[0m, in \u001b[0;36mLanguage.add_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    777\u001b[0m     bad_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrepr\u001b[39m(factory_name)\n\u001b[1;32m    778\u001b[0m     err \u001b[38;5;241m=\u001b[39m Errors\u001b[38;5;241m.\u001b[39mE966\u001b[38;5;241m.\u001b[39mformat(component\u001b[38;5;241m=\u001b[39mbad_val, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m--> 779\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err)\n\u001b[1;32m    780\u001b[0m name \u001b[38;5;241m=\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m factory_name\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent_names:\n",
      "\u001b[0;31mValueError\u001b[0m: [E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <spacy_ke.yake.Yake object at 0x7f530a7ba4c0> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import random\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import spacy_ke\n",
    "from spacy_ke import Yake\n",
    "\n",
    "\n",
    "target = \"The metal content of ore was increased through the process to make transportation less expensive.\"\n",
    "nlp = spacy.load('en_core_web_sm')# 加载预训练模型\n",
    "\n",
    "\n",
    "# nlp.add_pipe(\"yake\")\n",
    "\n",
    "nlp.add_pipe(\n",
    "    Yake(\n",
    "        nlp,\n",
    "        window=1,  # default\n",
    "        lemmatize=False,  # default\n",
    "        candidate_selection=\"ngram\"  # default, use \"chunk\" for noun phrase selection\n",
    "    )\n",
    ")\n",
    "nlp.add_pipe(\"yake\")\n",
    "\n",
    "doc = nlp(target)\n",
    "words = []\n",
    "\n",
    "for keyword, score in doc._.extract_keywords(n=3):\n",
    "    print(keyword, \"-\", score)\n",
    "    \n",
    "# for phrase in doc._.phrases:\n",
    "#     # print(phrase.rank, phrase.count)\n",
    "#     print(phrase.chunks[0])\n",
    "#     words.append(phrase.chunks[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py39]",
   "language": "python",
   "name": "conda-env-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
