{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c27bbae-fdcf-4e31-ba4c-5e39548022f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import transformers\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    BertTokenizer,\n",
    "    GPT2Tokenizer\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
    "from transformers import GPT2LMHeadModel, AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "model_name_or_path = \"/home2/zhanghanqing/pretrained_model/gpt2/large\"\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "            model_name_or_path)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "7ac9e34a-0b4f-47ae-a4db-18289f1c9198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_metric import cal_ppl_bygpt2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "text = [\"I love you\", \"love pretrain hello\"]\n",
    "\n",
    "ppl = cal_ppl_bygpt2(tokenizer, model, 20, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c7204bc5-909f-4313-bf68-4eaf66c084fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0.dev20221024+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "708a8258-75a3-4620-86ac-d4a5683b5b41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhanghanqing/anaconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import copy\n",
    "from typing import Optional, Any, Union, Callable\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import MultiheadAttention\n",
    "from torch.nn import Dropout\n",
    "from torch.nn import Linear\n",
    "from torch.nn import LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "883714ac-72c2-4bb6-9d65-ff8d4ceea099",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformerDecoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0md_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnhead\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdim_feedforward\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdropout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mactivation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mrelu\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x7f9f4525d3a0\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlayer_norm_eps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnorm_first\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.\n",
       "This standard decoder layer is based on the paper \"Attention Is All You Need\".\n",
       "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
       "Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
       "Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
       "in a different way during application.\n",
       "\n",
       "Args:\n",
       "    d_model: the number of expected features in the input (required).\n",
       "    nhead: the number of heads in the multiheadattention models (required).\n",
       "    dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
       "    dropout: the dropout value (default=0.1).\n",
       "    activation: the activation function of the intermediate layer, can be a string\n",
       "        (\"relu\" or \"gelu\") or a unary callable. Default: relu\n",
       "    layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n",
       "    batch_first: If ``True``, then the input and output tensors are provided\n",
       "        as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
       "    norm_first: if ``True``, layer norm is done prior to self attention, multihead\n",
       "        attention and feedforward operations, respectively. Otherwise it's done after.\n",
       "        Default: ``False`` (after).\n",
       "\n",
       "Examples::\n",
       "    >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
       "    >>> memory = torch.rand(10, 32, 512)\n",
       "    >>> tgt = torch.rand(20, 32, 512)\n",
       "    >>> out = decoder_layer(tgt, memory)\n",
       "\n",
       "Alternatively, when ``batch_first`` is ``True``:\n",
       "    >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8, batch_first=True)\n",
       "    >>> memory = torch.rand(32, 10, 512)\n",
       "    >>> tgt = torch.rand(32, 20, 512)\n",
       "    >>> out = decoder_layer(tgt, memory)\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/transformer.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     Transformer_Decoder\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?nn.TransformerDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b8572ad-e428-447b-91ad-9fbfd9cf7d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhanghanqing/anaconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from adapters.untitled  import Transformer_Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2f45e8c-14d2-4e09-b7d8-e163fa29dfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c21129b-387c-40c0-8e99-4caff433f34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "decoder_layer = Transformer_Decoder(d_model=512, nhead=8, batch_first=True)\n",
    "memory = torch.rand(32, 10, 512)\n",
    "tgt = torch.rand(32, 20, 512)\n",
    "out = decoder_layer(tgt, tgt, memory)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9a04fc76-6c05-466a-96c8-9ee9ae5135c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 15496,    11,   616,  3290,   318,   257,  1310,  1643,   286,\n",
      "           257,  2356,   287,   262,   840,    11,   475,   339,   338,   257],\n",
      "        [ 8206,   290,  2456,  2291,    25, 16569,    11,  9280,    11,  1620,\n",
      "            11,  3800,    11,  5806,    13,  7253,  5270,    25,   352,    12,\n",
      "            17,  2745,    13,   198,   198,    16,    12,    17,  2745,    13]])\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is a little bit of a pain in the ass, but he's a\n",
      "Text and words include: costume, dance, perform, stage, wear. Start generation: 1-2 weeks.\n",
      "\n",
      "1-2 weeks.\n"
     ]
    }
   ],
   "source": [
    "tokenizer.padding_side = \"left\" \n",
    "tokenizer.pad_token = tokenizer.eos_token # to avoid an error\n",
    "\n",
    "sentences = [\"Hello, my dog is a little\",\n",
    "            \"Text and words include: costume, dance, perform, stage, wear. Start generation:\"\n",
    "            ]\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "output_sequences = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    num_beams=3,\n",
    "    do_sample=False, \n",
    "    max_length=30 # disable sampling to test if batching affects output\n",
    ")\n",
    "print(output_sequences)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    print(tokenizer.decode(output_sequences[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d40d148-5be5-4729-a416-f5e561af7530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdo_sample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mearly_stopping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_beams\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtemperature\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtop_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtypical_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbad_words_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mforce_words_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbos_token_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpad_token_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0meos_token_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlength_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mno_repeat_ngram_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mencoder_no_repeat_ngram_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_time\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdecoder_start_token_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_cache\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_beam_groups\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdiversity_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprefix_allowed_tokens_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlogits_processor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_logits_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogitsProcessorList\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrenormalize_logits\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstopping_criteria\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_stopping_criteria\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStoppingCriteriaList\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconstraints\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_beam_constraints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConstraint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moutput_scores\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mforced_bos_token_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mforced_eos_token_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mremove_invalid_values\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msynced_gpus\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mexponential_decay_length_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msuppress_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbegin_suppress_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mforced_decoder_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGreedySearchEncoderDecoderOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGreedySearchDecoderOnlyOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSampleEncoderDecoderOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSampleDecoderOnlyOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBeamSearchEncoderDecoderOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBeamSearchDecoderOnlyOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBeamSampleEncoderDecoderOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBeamSampleDecoderOnlyOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Generates sequences of token ids for models with a language modeling head. The method supports the following\n",
       "generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:\n",
       "\n",
       "    - *greedy decoding* by calling [`~generation_utils.GenerationMixin.greedy_search`] if `num_beams=1` and\n",
       "      `do_sample=False`.\n",
       "    - *multinomial sampling* by calling [`~generation_utils.GenerationMixin.sample`] if `num_beams=1` and\n",
       "      `do_sample=True`.\n",
       "    - *beam-search decoding* by calling [`~generation_utils.GenerationMixin.beam_search`] if `num_beams>1` and\n",
       "      `do_sample=False`.\n",
       "    - *beam-search multinomial sampling* by calling [`~generation_utils.GenerationMixin.beam_sample`] if\n",
       "      `num_beams>1` and `do_sample=True`.\n",
       "    - *diverse beam-search decoding* by calling [`~generation_utils.GenerationMixin.group_beam_search`], if\n",
       "      `num_beams>1` and `num_beam_groups>1`.\n",
       "    - *constrained beam-search decoding* by calling\n",
       "      [`~generation_utils.GenerationMixin.constrained_beam_search`], if `constraints!=None` or\n",
       "      `force_words_ids!=None`.\n",
       "\n",
       "<Tip warning={true}>\n",
       "\n",
       "Apart from `inputs`, all the arguments below will default to the value of the attribute of the same name as\n",
       "defined in the model's config (`config.json`) which in turn defaults to the\n",
       "[`~modeling_utils.PretrainedConfig`] of the model.\n",
       "\n",
       "</Tip>\n",
       "\n",
       "Most of these parameters are explained in more detail in [this blog\n",
       "post](https://huggingface.co/blog/how-to-generate).\n",
       "\n",
       "Parameters:\n",
       "    inputs (`torch.Tensor` of varying shape depending on the modality, *optional*):\n",
       "        The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the\n",
       "        method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`\n",
       "        should of in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of\n",
       "        `input_ids`, `input_values`, `input_features`, or `pixel_values`.\n",
       "    max_length (`int`, *optional*, defaults to `model.config.max_length`):\n",
       "        The maximum length the generated tokens can have. Corresponds to the length of the input prompt +\n",
       "        `max_new_tokens`. In general, prefer the use of `max_new_tokens`, which ignores the number of tokens in\n",
       "        the prompt.\n",
       "    max_new_tokens (`int`, *optional*):\n",
       "        The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
       "    min_length (`int`, *optional*, defaults to `model.config.min_length` or 10 if the config does not set any value):\n",
       "        The minimum length of the sequence to be generated.\n",
       "    do_sample (`bool`, *optional*, defaults to `model.config.do_sample` or `False` if the config does not set any value):\n",
       "        Whether or not to use sampling ; use greedy decoding otherwise.\n",
       "    early_stopping (`bool`, *optional*, defaults to `False`):\n",
       "        Whether to stop the beam search when at least `num_beams` sentences are finished per batch or not.\n",
       "    num_beams (`int`, *optional*, defaults to `model.config.num_beams` or 1 if the config does not set any value):\n",
       "        Number of beams for beam search. 1 means no beam search.\n",
       "    temperature (`float`, *optional*, defaults to `model.config.temperature` or 1.0 if the config does not set any value):\n",
       "        The value used to module the next token probabilities.\n",
       "    top_k (`int`, *optional*, defaults to `model.config.top_k` or 50 if the config does not set any value):\n",
       "        The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
       "    top_p (`float`, *optional*, defaults to `model.config.top_p` or 1.0 if the config does not set any value):\n",
       "        If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to\n",
       "        `top_p` or higher are kept for generation.\n",
       "    typical_p (`float`, *optional*, defaults to `model.config.typical_p` or 1.0 if the config does not set any value):\n",
       "        The amount of probability mass from the original distribution to be considered in typical decoding. If\n",
       "        set to 1.0 it takes no effect. See [this paper](https://arxiv.org/pdf/2202.00666.pdf) for more details.\n",
       "    repetition_penalty (`float`, *optional*, defaults to `model.config.repetition_penalty` or 1.0 if the config does not set any value):\n",
       "        The parameter for repetition penalty. 1.0 means no penalty. See [this\n",
       "        paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.\n",
       "    pad_token_id (`int`, *optional*, defaults to `model.config.pad_token_id`):\n",
       "        The id of the *padding* token.\n",
       "    bos_token_id (`int`, *optional*, defaults to `model.config.bos_token_id`):\n",
       "        The id of the *beginning-of-sequence* token.\n",
       "    eos_token_id (`int`, *optional*, defaults to `model.config.eos_token_id`):\n",
       "        The id of the *end-of-sequence* token.\n",
       "    length_penalty (`float`, *optional*, defaults to `model.config.length_penalty` or 1.0 if the config does not set any value):\n",
       "        Exponential penalty to the length that is used with beam-based generation. It is applied as an exponent\n",
       "        to the sequence length, which in turn is used to divide the score of the sequence. Since the score is\n",
       "        the log likelihood of the sequence (i.e. negative), `length_penalty` > 0.0 promotes longer sequences,\n",
       "        while `length_penalty` < 0.0 encourages shorter sequences.\n",
       "    no_repeat_ngram_size (`int`, *optional*, defaults to `model.config.no_repeat_ngram_size` or 0 if the config does not set any value):\n",
       "        If set to int > 0, all ngrams of that size can only occur once.\n",
       "    encoder_no_repeat_ngram_size (`int`, *optional*, defaults to `model.config.encoder_no_repeat_ngram_size` or 0 if the config does not set any value):\n",
       "        If set to int > 0, all ngrams of that size that occur in the `encoder_input_ids` cannot occur in the\n",
       "        `decoder_input_ids`.\n",
       "    bad_words_ids(`List[List[int]]`, *optional*, defaults to `model.config.bad_words_ids`):\n",
       "        List of token ids that are not allowed to be generated. In order to get the token ids of the words that\n",
       "        should not appear in the generated text, use `tokenizer(bad_words, add_prefix_space=True,\n",
       "        add_special_tokens=False).input_ids`.\n",
       "    force_words_ids(`List[List[int]]` or `List[List[List[int]]]`, *optional*):\n",
       "        List of token ids that must be generated. If given a `List[List[int]]`, this is treated as a simple\n",
       "        list of words that must be included, the opposite to `bad_words_ids`. If given `List[List[List[int]]]`,\n",
       "        this triggers a [disjunctive constraint](https://github.com/huggingface/transformers/issues/14081),\n",
       "        where one can allow different forms of each word.\n",
       "    num_return_sequences(`int`, *optional*, defaults to `model.config.num_return_sequences` or 1 if the config does not set any value):\n",
       "        The number of independently computed returned sequences for each element in the batch.\n",
       "    max_time(`float`, *optional*):\n",
       "        The maximum amount of time you allow the computation to run for in seconds. generation will still\n",
       "        finish the current pass after allocated time has been passed.\n",
       "    attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
       "        Mask to avoid performing attention on padding token indices. Mask values are in `[0, 1]`, 1 for tokens\n",
       "        that are not masked, and 0 for masked tokens. If not provided, will default to a tensor the same shape\n",
       "        as `input_ids` that masks the pad token. [What are attention masks?](../glossary#attention-mask)\n",
       "    decoder_start_token_id (`int`, *optional*):\n",
       "        If an encoder-decoder model starts decoding with a different token than *bos*, the id of that token.\n",
       "    use_cache (`bool`, *optional*, defaults to `True`):\n",
       "        Whether or not the model should use the past last key/values attentions (if applicable to the model) to\n",
       "        speed up decoding.\n",
       "    num_beam_groups (`int`, *optional*, defaults to `model.config.num_beam_groups` or 1 if the config does not set any value):\n",
       "        Number of groups to divide `num_beams` into in order to ensure diversity among different groups of\n",
       "        beams. [this paper](https://arxiv.org/pdf/1610.02424.pdf) for more details.\n",
       "    diversity_penalty (`float`, *optional*, defaults to `model.config.diversity_penalty` or 0.0 if the config does not set any value):\n",
       "        This value is subtracted from a beam's score if it generates a token same as any beam from other group\n",
       "        at a particular time. Note that `diversity_penalty` is only effective if `group beam search` is\n",
       "        enabled.\n",
       "    prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n",
       "        If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
       "        provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n",
       "        `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n",
       "        on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n",
       "        for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n",
       "        Retrieval](https://arxiv.org/abs/2010.00904).\n",
       "    logits_processor (`LogitsProcessorList`, *optional*):\n",
       "         Custom logits processors that complement the default logits processors built from arguments and a\n",
       "         model's config. If a logit processor is passed that is already created with the arguments or a model's\n",
       "         config an error is thrown. This feature is intended for advanced users.\n",
       "    renormalize_logits: (`bool`, *optional*, defaults to `False`):\n",
       "        Whether to renormalize the logits after applying all the logits processors or warpers (including the\n",
       "        custom ones). It's highly recommended to set this flag to `True` as the search algorithms suppose the\n",
       "        score logits are normalized but some logit processors or warpers break the normalization.\n",
       "    stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
       "         Custom stopping criteria that complement the default stopping criteria built from arguments and a\n",
       "         model's config. If a stopping criteria is passed that is already created with the arguments or a\n",
       "         model's config an error is thrown. This feature is intended for advanced users.\n",
       "    constraints (`List[Constraint]`, *optional*):\n",
       "         Custom constraints that can be added to the generation to ensure that the output will contain the use\n",
       "         of certain tokens as defined by `Constraint` objects, in the most sensible way possible.\n",
       "    output_attentions (`bool`, *optional*, defaults to `model.config.output_attentions` or `False` if the config does not set any value):\n",
       "        Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
       "        returned tensors for more details.\n",
       "    output_hidden_states (`bool`, *optional*, defaults to `model.config.output_hidden_states` or `False` if the config does not set any value):\n",
       "        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
       "        for more details.\n",
       "    output_scores (`bool`, *optional*, defaults to `model.config.output_scores` or `False` if the config does not set any value):\n",
       "        Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
       "    return_dict_in_generate (`bool`, *optional*, defaults to `model.config.return_dict_in_generate` or `False` if the config does not set any value):\n",
       "        Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
       "    forced_bos_token_id (`int`, *optional*, defaults to `model.config.forced_bos_token_id`):\n",
       "        The id of the token to force as the first generated token after the `decoder_start_token_id`. Useful\n",
       "        for multilingual models like [mBART](../model_doc/mbart) where the first generated token needs to be\n",
       "        the target language token.\n",
       "    forced_eos_token_id (`int`, *optional*, defaults to `model.config.forced_eos_token_id`):\n",
       "        The id of the token to force as the last generated token when `max_length` is reached.\n",
       "    remove_invalid_values (`bool`, *optional*, defaults to `model.config.remove_invalid_values`):\n",
       "        Whether to remove possible *nan* and *inf* outputs of the model to prevent the generation method to\n",
       "        crash. Note that using `remove_invalid_values` can slow down generation.\n",
       "    synced_gpus (`bool`, *optional*, defaults to `False`):\n",
       "        Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
       "    exponential_decay_length_penalty (`tuple(int, float)`, *optional*, defaults to `model.config.exponential_decay_length_penalty`):\n",
       "        This Tuple adds an exponentially increasing length penalty, after a certain amount of tokens have been\n",
       "        generated. The tuple shall consist of: `(start_index, decay_factor)` where `start_index` indicates\n",
       "        where penalty starts and `decay_factor` represents the factor of exponential decay\n",
       "    suppress_tokens  (`List[int]`, *optional*, defaults to `model.config.suppress_tokens`):\n",
       "        A list of tokens that will be supressed at generation. The `SupressTokens` logit processor will set\n",
       "        their log probs to `-inf` so that they are not sampled.\n",
       "    begin_suppress_tokens  (`List[int]`, *optional*, defaults to `model.config.begin_suppress_tokens`):\n",
       "        A list of tokens that will be supressed at the begining of the generation. The `SupressBeginTokens`\n",
       "        logit processor will set their log probs to `-inf` so that they are not sampled.\n",
       "    forced_decoder_ids (`List[int]`, *optional*, defaults to `model.config.forced_decoder_ids`):\n",
       "        A list of tokens that will be forced as beginning tokens, before sampling.\n",
       "\n",
       "    model_kwargs:\n",
       "        Additional model specific kwargs will be forwarded to the `forward` function of the model. If the model\n",
       "        is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs\n",
       "        should be prefixed with *decoder_*.\n",
       "\n",
       "Return:\n",
       "    [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n",
       "    or when `config.return_dict_in_generate=True`) or a `torch.FloatTensor`.\n",
       "\n",
       "        If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible\n",
       "        [`~utils.ModelOutput`] types are:\n",
       "\n",
       "            - [`~generation_utils.GreedySearchDecoderOnlyOutput`],\n",
       "            - [`~generation_utils.SampleDecoderOnlyOutput`],\n",
       "            - [`~generation_utils.BeamSearchDecoderOnlyOutput`],\n",
       "            - [`~generation_utils.BeamSampleDecoderOnlyOutput`]\n",
       "\n",
       "        If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible\n",
       "        [`~utils.ModelOutput`] types are:\n",
       "\n",
       "            - [`~generation_utils.GreedySearchEncoderDecoderOutput`],\n",
       "            - [`~generation_utils.SampleEncoderDecoderOutput`],\n",
       "            - [`~generation_utils.BeamSearchEncoderDecoderOutput`],\n",
       "            - [`~generation_utils.BeamSampleEncoderDecoderOutput`]\n",
       "\n",
       "Examples:\n",
       "\n",
       "Greedy Decoding:\n",
       "\n",
       "```python\n",
       ">>> from transformers import AutoTokenizer, AutoModelForCausalLM\n",
       "\n",
       ">>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
       ">>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
       "\n",
       ">>> prompt = \"Today I believe we can finally\"\n",
       ">>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
       "\n",
       ">>> # generate up to 30 tokens\n",
       ">>> outputs = model.generate(input_ids, do_sample=False, max_length=30)\n",
       ">>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
       "['Today I believe we can finally get to the point where we can make a difference in the lives of the people of the United States of America.\\n']\n",
       "```\n",
       "\n",
       "Multinomial Sampling:\n",
       "\n",
       "```python\n",
       ">>> from transformers import AutoTokenizer, AutoModelForCausalLM\n",
       ">>> import torch\n",
       "\n",
       ">>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
       ">>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
       "\n",
       ">>> prompt = \"Today I believe we can finally\"\n",
       ">>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
       "\n",
       ">>> # sample up to 30 tokens\n",
       ">>> torch.manual_seed(0)  # doctest: +IGNORE_RESULT\n",
       ">>> outputs = model.generate(input_ids, do_sample=True, max_length=30)\n",
       ">>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
       "['Today I believe we can finally get rid of discrimination,\" said Rep. Mark Pocan (D-Wis.).\\n\\n\"Just look at the']\n",
       "```\n",
       "\n",
       "Beam-search decoding:\n",
       "\n",
       "```python\n",
       ">>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
       "\n",
       ">>> tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
       ">>> model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
       "\n",
       ">>> sentence = \"Paris is one of the densest populated areas in Europe.\"\n",
       ">>> input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
       "\n",
       ">>> outputs = model.generate(input_ids, num_beams=5)\n",
       ">>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
       "['Paris ist eines der dichtesten besiedelten Gebiete Europas.']\n",
       "```\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/generation_utils.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?GPT2LMHeadModel.generate\n",
    "\n",
    "# repetition_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f627624f-ea4b-439f-acfb-891a1047e915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 4, 6, 8],\n",
      "        [2, 4, 6, 8]])\n"
     ]
    }
   ],
   "source": [
    "c=d+d\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "669cdbe3-b62a-4339-838f-e02234004362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "import nltk.translate.bleu_score as nltkbleu\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import spacy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def nltk_bleu(refs, pred, n):\n",
    "    \"\"\"\n",
    "    一般smoothing_function选择默认即可；\n",
    "    默认n=4\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = [v[0] for k,v in res.items()]\n",
    "    references = [v for k,v in gts.items()]\n",
    "    \n",
    "    refs=[]\n",
    "    \n",
    "    refs = [tokenizer(references) for ref in refs]\n",
    "    pred = tokenizer(predictions)\n",
    "    \n",
    "    weights = [1 / n for _ in range(n)]\n",
    "    score = sentence_bleu(\n",
    "        refs,\n",
    "        pred,\n",
    "        smoothing_function=nltkbleu.SmoothingFunction().method7,\n",
    "        weights=weights\n",
    "    )\n",
    "    print(score)\n",
    "    \n",
    "\n",
    "\n",
    "def calculateSelfBLEU(texts, ngram=5):\n",
    "    if len(texts) == 1:\n",
    "        return 0\n",
    "\n",
    "    spacyTexts = list(nlp.pipe(texts))\n",
    "    textsSplits = np.array([[token.text for token in t] for t in spacyTexts], dtype=object)\n",
    "\n",
    "    arange = np.arange(len(textsSplits))\n",
    "    weights = tuple((1. / ngram for _ in range(ngram)))\n",
    "\n",
    "    pool = Pool(os.cpu_count())\n",
    "    bleus = list()\n",
    "    for idx, candidate in enumerate(textsSplits):\n",
    "        reference = textsSplits[arange != idx].tolist()\n",
    "        bleus.append(pool.apply_async(calcBleu, args=(reference, candidate, weights)))\n",
    "\n",
    "    for idx, b in tqdm(enumerate(bleus), total=len(bleus)):\n",
    "        bleus[idx] = b.get()\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    return np.mean(bleus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9e593f4-65eb-4c82-8b96-7bcf576f82bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = {\n",
    "\"1\":[\"Today is a nice days\"],\n",
    "\"2\":[\"I like  huggingface, it is easy to use\"],\n",
    "\"3\":[\"big model is changing the worlds\"]\n",
    "}\n",
    "\n",
    "gts = {\n",
    "\"1\":[\"Today is a nice day\"],\n",
    "\"2\":[\"I like huggingface, it is easy to use\"],\n",
    "\"3\":[\"big model is changing the worlds\"]\n",
    "}\n",
    "\n",
    "# ref = [v for k, v in reference.items()]\n",
    "\n",
    "# cand = [v for k, v in text.items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "885edf37-b05e-4fba-85fb-ece4b40bb309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': ['today is a nice day'], '2': ['i like huggingface it is easy to use'], '3': ['big model is changing the worlds']} {'1': ['today is a nice days'], '2': ['i like huggingface it is easy to use'], '3': ['big model is changing the world']}\n"
     ]
    }
   ],
   "source": [
    "from speaksee import evaluation\n",
    "\n",
    "def tokenize(dict):\n",
    "    for key in dict:\n",
    "        new_sentence_list = []\n",
    "        for sentence in dict[key]:\n",
    "            a = ''\n",
    "            for token in nlp(sentence):\n",
    "                a += token.text\n",
    "                a += ' '\n",
    "            new_sentence_list.append(a.rstrip())\n",
    "        dict[key] = new_sentence_list\n",
    "\n",
    "    return dict\n",
    "\n",
    "# gts = tokenize(gts)\n",
    "# gen = tokenize(gen)\n",
    "\n",
    "gts = evaluation.PTBTokenizer.tokenize(gts)\n",
    "gen = evaluation.PTBTokenizer.tokenize(gen)\n",
    "\n",
    "\n",
    "print(gts,gen)\n",
    "# val_bleu, _ = evaluation.Bleu(n=4).compute_score(gts, gen)\n",
    "# method = ['Blue_1', 'Bleu_2', 'Bleu_3', 'Bleu_4']\n",
    "# metric_dict = {}\n",
    "# for metric, score in zip(method, val_bleu):\n",
    "#     metric_dict[metric] = {'entire': score * 100}\n",
    "#     print('%s %.2f' % (metric, score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf07bc0c-3c9e-402a-bf0d-449519e59481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['today is a nice day'] ['today is a nice days']\n",
      "['i like huggingface it is easy to use'] ['i like huggingface it is easy to use']\n",
      "['big model is changing the worlds'] ['big model is changing the world']\n",
      "NIST 3.85\n",
      "BLEU 85.32\n"
     ]
    }
   ],
   "source": [
    "from eval_metric import BLEUScore, NISTScore\n",
    "\n",
    "bleu = BLEUScore()\n",
    "nist = NISTScore()\n",
    "\n",
    "for sents_ref, sent_sys in zip(gts.values(), gen.values()):\n",
    "    print(sents_ref, sent_sys)\n",
    "    \n",
    "    bleu.append(sent_sys[0], sents_ref)\n",
    "    nist.append(sent_sys[0], sents_ref)\n",
    "    \n",
    "print(\"NIST %.2f\" % (nist.score()))\n",
    "print(\"BLEU %.2f\" % (bleu.score() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "fa67331a-7bc0-41bb-91f3-faefb90b26e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Today', 'is', 'a', 'nice', 'day']]\n",
      "[['I', 'like', 'huggingface,', 'it', 'is', 'easy', 'to', 'use']]\n",
      "[['big', 'model', 'is', 'changing', 'the', 'world']]\n",
      "[0.7745966692414834, 0.6976750600527641, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bigram': 0.8240905764314158,\n",
       " 'trigram': 0.7785575236143242,\n",
       " 'quagram': 0.7311587009123848}"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fast_bleu import BLEU, SelfBLEU\n",
    "import numpy as np\n",
    "\n",
    "evaluator_bleu(text, reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "305be2c2-1146-4c99-9b7d-2814cf38bf73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['big', 'model', 'is', 'changing', 'the', 'world']"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference[\"3\"][0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1343487-8351-4155-a73b-0b7f7bf65394",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,\"mask\",\"mask\",5,6,7,8,\"mask\",10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b4cb2f76-65b7-43c8-8df0-32d891a2b007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1, 2]\n",
      "[1, 2, 3]\n",
      "[1, 2, 3, 'mask']\n",
      "[1, 2, 3, 'mask']\n",
      "[1, 2, 3, 'mask', 5]\n",
      "[1, 2, 3, 'mask', 5, 6]\n",
      "[1, 2, 3, 'mask', 5, 6, 7]\n",
      "[1, 2, 3, 'mask', 5, 6, 7, 8]\n",
      "[1, 2, 3, 'mask', 5, 6, 7, 8, 'mask']\n",
      "[1, 2, 3, 'mask', 5, 6, 7, 8, 'mask', 10]\n"
     ]
    }
   ],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c16fb73f-9c97-4e65-8ea4-99e6eb15d13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 'mask', 5, 6, 7, 8, 'mask', 10]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d5ad63eb-f138-4a44-a1a1-bdfdb838d794",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (449295825.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [81]\u001b[0;36m\u001b[0m\n\u001b[0;31m    a 6\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "a 6\n",
    "if a%6 < 6:\n",
    "    print(\"hello\")\n",
    "elif a %6==1:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9055129d-d955-43cf-8226-dd0b6c07a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_set_input_ids =[1,2,3,4,5,6,7,7,9,0]\n",
    "length = len(concept_set_input_ids)\n",
    "sentinal_count = max(int(length * 0.2), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4d50ebcc-733a-4c8b-8bca-c3947e34a6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentinal_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ea03acdf-f2cd-4686-b535-9559ee754a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1026dbfe-1b87-4ece-9560-1fb1822736e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py39]",
   "language": "python",
   "name": "conda-env-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
