{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c27bbae-fdcf-4e31-ba4c-5e39548022f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import transformers\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    BertTokenizer,\n",
    "    GPT2Tokenizer\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
    "from transformers import GPT2LMHeadModel, AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "model_name_or_path = \"/home2/zhanghanqing/pretrained_model/gpt2/large\"\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "            model_name_or_path)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2733f4a0-05e0-41b6-8e82-17aa461d0fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33325704-aa65-43fe-92ce-c57e097e5155",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only assign an iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a[:\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only assign an iterable"
     ]
    }
   ],
   "source": [
    "a[:2]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3848fbd-38b2-42c1-8909-1772d862b339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [], 'attention_mask': []}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7a23761-f38e-45b4-9d5d-ea04a0896f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [20342, 11, 23748, 995], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"hey, hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4104c5e3-77d1-44b8-bc3c-99e0ff574211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [23748, 995], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\" hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "7ac9e34a-0b4f-47ae-a4db-18289f1c9198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_metric import cal_ppl_bygpt2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "text = [\"I love you\", \"love pretrain hello\"]\n",
    "\n",
    "ppl = cal_ppl_bygpt2(tokenizer, model, 20, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efad93ea-3c70-4c73-a1b6-c9c08fec07af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [464], 'attention_mask': [1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"The\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c7204bc5-909f-4313-bf68-4eaf66c084fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0.dev20221024+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "708a8258-75a3-4620-86ac-d4a5683b5b41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhanghanqing/anaconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import copy\n",
    "from typing import Optional, Any, Union, Callable\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import MultiheadAttention\n",
    "from torch.nn import Dropout\n",
    "from torch.nn import Linear\n",
    "from torch.nn import LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "883714ac-72c2-4bb6-9d65-ff8d4ceea099",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformerDecoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0md_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnhead\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdim_feedforward\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdropout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mactivation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mrelu\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x7f9f4525d3a0\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlayer_norm_eps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnorm_first\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.\n",
       "This standard decoder layer is based on the paper \"Attention Is All You Need\".\n",
       "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
       "Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
       "Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
       "in a different way during application.\n",
       "\n",
       "Args:\n",
       "    d_model: the number of expected features in the input (required).\n",
       "    nhead: the number of heads in the multiheadattention models (required).\n",
       "    dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
       "    dropout: the dropout value (default=0.1).\n",
       "    activation: the activation function of the intermediate layer, can be a string\n",
       "        (\"relu\" or \"gelu\") or a unary callable. Default: relu\n",
       "    layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n",
       "    batch_first: If ``True``, then the input and output tensors are provided\n",
       "        as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
       "    norm_first: if ``True``, layer norm is done prior to self attention, multihead\n",
       "        attention and feedforward operations, respectively. Otherwise it's done after.\n",
       "        Default: ``False`` (after).\n",
       "\n",
       "Examples::\n",
       "    >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
       "    >>> memory = torch.rand(10, 32, 512)\n",
       "    >>> tgt = torch.rand(20, 32, 512)\n",
       "    >>> out = decoder_layer(tgt, memory)\n",
       "\n",
       "Alternatively, when ``batch_first`` is ``True``:\n",
       "    >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8, batch_first=True)\n",
       "    >>> memory = torch.rand(32, 10, 512)\n",
       "    >>> tgt = torch.rand(32, 20, 512)\n",
       "    >>> out = decoder_layer(tgt, memory)\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/transformer.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     Transformer_Decoder\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?nn.TransformerDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b8572ad-e428-447b-91ad-9fbfd9cf7d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhanghanqing/anaconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from adapters.untitled  import Transformer_Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2f45e8c-14d2-4e09-b7d8-e163fa29dfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c21129b-387c-40c0-8e99-4caff433f34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "decoder_layer = Transformer_Decoder(d_model=512, nhead=8, batch_first=True)\n",
    "memory = torch.rand(32, 10, 512)\n",
    "tgt = torch.rand(32, 20, 512)\n",
    "out = decoder_layer(tgt, tgt, memory)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f6217-8767-4a5c-9e78-68af520afdf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9a04fc76-6c05-466a-96c8-9ee9ae5135c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 15496,    11,   616,  3290,   318,   257,  1310,  1643,   286,\n",
      "           257,  2356,   287,   262,   840,    11,   475,   339,   338,   257],\n",
      "        [ 8206,   290,  2456,  2291,    25, 16569,    11,  9280,    11,  1620,\n",
      "            11,  3800,    11,  5806,    13,  7253,  5270,    25,   352,    12,\n",
      "            17,  2745,    13,   198,   198,    16,    12,    17,  2745,    13]])\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is a little bit of a pain in the ass, but he's a\n",
      "Text and words include: costume, dance, perform, stage, wear. Start generation: 1-2 weeks.\n",
      "\n",
      "1-2 weeks.\n"
     ]
    }
   ],
   "source": [
    "tokenizer.padding_side = \"left\" \n",
    "tokenizer.pad_token = tokenizer.eos_token # to avoid an error\n",
    "\n",
    "sentences = [\"Hello, my dog is a little\",\n",
    "            \"Text and words include: costume, dance, perform, stage, wear. Start generation:\"\n",
    "            ]\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "output_sequences = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    num_beams=3,\n",
    "    do_sample=False, \n",
    "    max_length=30 # disable sampling to test if batching affects output\n",
    ")\n",
    "print(output_sequences)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    print(tokenizer.decode(output_sequences[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d40d148-5be5-4729-a416-f5e561af7530",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data from /home2/zhanghanqing/formatted_wikipedia ...\n",
      "data num is: 640000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 49/49 [03:01<00:00,  3.71s/it]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import json\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import random\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def encoder_data(data):\n",
    "    \n",
    "    nn = ['VBG', 'NOUN']\n",
    "    nlp = spacy.load('en_core_web_sm')# 加载预训练模型\n",
    "\n",
    "    res = []\n",
    "    \n",
    "    for line in data:\n",
    "        \n",
    "        cut_pos = random.randint(9,15)\n",
    "        line = line.strip(' \\n\\t\\r),\\\\')\n",
    "        \n",
    "        split_line= line.split()\n",
    "        \n",
    "        if cut_pos>len(split_line) or len(split_line)>150 or len(split_line)<3:\n",
    "            continue\n",
    "        \n",
    "        line_content  = ' '.join(split_line[:-cut_pos])\n",
    "        target = ' '.join(split_line[-cut_pos:])\n",
    "                \n",
    "        doc = nlp(target)\n",
    "        tokens = [token for token in doc]           # 将句子切分成单词\n",
    "        pos = [token.pos_ for token in doc]         # 词性标注\n",
    "        lem = [token.lemma_ for token in doc]       # 词性还原\n",
    "        \n",
    "        sen = []\n",
    "        for t,p,l in zip(tokens, pos, lem):\n",
    "            if p in nn:\n",
    "                sen.append(str(l))\n",
    "                \n",
    "        sen = list(set(sen))\n",
    "        \n",
    "        if len(sen)<3:\n",
    "            continue\n",
    "            \n",
    "        if len(sen)>5:\n",
    "            \n",
    "            sen =  random.sample(sen, 5)\n",
    "            \n",
    "        random.shuffle(sen)\n",
    " \n",
    "        res.append({\n",
    "                    \"content\":line_content,\n",
    "                    \"target\":target ,\n",
    "                    \"keywords\": '#'.join(sen)})\n",
    "        \n",
    "    return res \n",
    "\n",
    "\n",
    "json_path = '/home2/zhanghanqing/formatted_wikipedia'\n",
    "\n",
    "out_file_train = os.path.join('./data/', 'wiki_train.json')\n",
    "out_file_train = open(out_file_train, 'w', encoding='utf8')\n",
    "\n",
    "\n",
    "out_file_val = os.path.join('./data/', 'wiki_val.json')\n",
    "out_file_val = open(out_file_val, 'w', encoding='utf8')\n",
    "\n",
    "\n",
    "\n",
    "print(\"reading data from %s ...\" % json_path)\n",
    "\n",
    "filenames = []                \n",
    "filenames += glob(os.path.join(json_path,'wiki**.format'))\n",
    "\n",
    "data =  []\n",
    "\n",
    "for data_file in filenames:\n",
    "    data +=[x for x in Path(data_file).open().readlines()[-40000:]]\n",
    "\n",
    "print(\"data num is:\", len(data))\n",
    "\n",
    "n = int(len(data)/48)\n",
    "data_list =[data[i:i + n] for i in range(0, len(data), n)]\n",
    "\n",
    "with Pool(48) as p:\n",
    "\n",
    "    data =list(tqdm(p.imap(encoder_data, data_list), total=len(data_list)))\n",
    "\n",
    "record = [item for subl in data for item in subl]\n",
    "\n",
    "random.shuffle(record)\n",
    "\n",
    "out_file_train.write(json.dumps(record[:int(len(record)*0.95)])+'\\n')\n",
    "out_file_train.flush()\n",
    "\n",
    "out_file_val.write(json.dumps(record[int(len(record)*0.95):])+'\\n')\n",
    "out_file_val.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f627624f-ea4b-439f-acfb-891a1047e915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"./data/wiki_train.json\") as out:\n",
    "\n",
    "       lines = json.load(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98ae5dad-c70f-4458-b9b3-918a67a1b74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'Manningham, working with James',\n",
       " 'target': 'Douglas, showed that these were pieces of adult and not of young rabbits, and that Toft was not parturient.',\n",
       " 'keywords': 'rabbit#piece#adult'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "840dd2e7-564e-41ca-b7f1-021a549803cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1}\n"
     ]
    }
   ],
   "source": [
    "a = set()\n",
    "a.add(1)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65bac06f-a4cf-4619-bc48-b45f5a6767a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the cake, top with sugar free powder sprinkles.\n"
     ]
    }
   ],
   "source": [
    "a = \"For the cake, top with sugar-free powder sprinkles.\"\n",
    "b = a.replace(\"-\",' ')\n",
    "print(b)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "def tokenize(dict):\n",
    "    for key in dict:\n",
    "        new_sentence_list = []\n",
    "        for sentence in dict[key]:\n",
    "            a = ''\n",
    "            for token in nlp(sentence):\n",
    "                a += token.text\n",
    "                a += ' '\n",
    "            new_sentence_list.append(a.rstrip())\n",
    "        dict[key] = new_sentence_list\n",
    "\n",
    "    return dict\n",
    "\n",
    "def get_coverage_score(gt_concepts, pred):\n",
    "    covs = []\n",
    "    total_cs, match_cs = 0, 0\n",
    "    for cs, p in zip(gt_concepts, pred):\n",
    "        p = p.lower()\n",
    "        if p.endswith('.'):\n",
    "            p = p[:-1]\n",
    "            p = p.strip()\n",
    "        cs = set(cs)\n",
    "        lemmas = set()\n",
    "        for token in nlp(p):\n",
    "            lemmas.add(token.lemma_)\n",
    "        match_cs += len(lemmas&cs)\n",
    "        total_cs += len(cs)\n",
    "        cov = len(lemmas&cs)/len(cs)\n",
    "        covs.append(cov)\n",
    "    return 100 * sum(covs) / len(covs), 100 * match_cs / total_cs\n",
    "\n",
    "\n",
    "\n",
    "def evaluator_coverage(res):\n",
    "    \n",
    "    pred = tokenize(res)\n",
    "    gt_concepts = [r.split('#') for r in list(pred.keys())]\n",
    "    \n",
    "    predictions = [v[0] for k,v in pred.items()]\n",
    "    \n",
    "    print(gt_concepts)\n",
    "    print(predictions)\n",
    "\n",
    "    score = get_coverage_score(gt_concepts, predictions)\n",
    "    print(score)\n",
    "    \n",
    "    return mean(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81612647-f0f0-4a9b-a6a9-0340d6a1a9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sugar', 'free', 'powder']]\n",
      "['For the cake , top with sugar - free powder sprinkle .']\n",
      "(100.0, 100.0)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m res\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msugar#free#powder\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor the cake, top with sugar-free powder sprinkle.\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m----> 3\u001b[0m \u001b[43mevaluator_coverage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36mevaluator_coverage\u001b[0;34m(res)\u001b[0m\n\u001b[1;32m     51\u001b[0m score \u001b[38;5;241m=\u001b[39m get_coverage_score(gt_concepts, predictions)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(score)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmean\u001b[49m(score)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean' is not defined"
     ]
    }
   ],
   "source": [
    "res={\"sugar#free#powder\": [\"For the cake, top with sugar-free powder sprinkle.\"]}\n",
    "\n",
    "evaluator_coverage(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1ed53b5-c819-490f-a390-866f405fc5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F',\n",
       " 'o',\n",
       " 'r',\n",
       " '',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " '',\n",
       " 'c',\n",
       " 'a',\n",
       " 'k',\n",
       " 'e',\n",
       " ',',\n",
       " '',\n",
       " 't',\n",
       " 'o',\n",
       " 'p',\n",
       " '',\n",
       " 'w',\n",
       " 'i',\n",
       " 't',\n",
       " 'h',\n",
       " '',\n",
       " 's',\n",
       " 'u',\n",
       " 'g',\n",
       " 'a',\n",
       " 'r',\n",
       " '-',\n",
       " 'f',\n",
       " 'r',\n",
       " 'e',\n",
       " 'e',\n",
       " '',\n",
       " 'p',\n",
       " 'o',\n",
       " 'w',\n",
       " 'd',\n",
       " 'e',\n",
       " 'r',\n",
       " '',\n",
       " 's',\n",
       " 'p',\n",
       " 'r',\n",
       " 'i',\n",
       " 'n',\n",
       " 'k',\n",
       " 'l',\n",
       " 'e',\n",
       " 's',\n",
       " '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"sugar#free#powder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9e593f4-65eb-4c82-8b96-7bcf576f82bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = {\n",
    "\"1\":[\"Today is a nice days\"],\n",
    "\"2\":[\"I like  huggingface, it is easy to use\"],\n",
    "\"3\":[\"big model is changing the worlds\"]\n",
    "}\n",
    "\n",
    "gts = {\n",
    "\"1\":[\"Today is a nice day\"],\n",
    "\"2\":[\"I like huggingface, it is easy to use\"],\n",
    "\"3\":[\"big model is changing the worlds\"]\n",
    "}\n",
    "\n",
    "# ref = [v for k, v in reference.items()]\n",
    "\n",
    "# cand = [v for k, v in text.items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "885edf37-b05e-4fba-85fb-ece4b40bb309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': ['today is a nice day'], '2': ['i like huggingface it is easy to use'], '3': ['big model is changing the worlds']} {'1': ['today is a nice days'], '2': ['i like huggingface it is easy to use'], '3': ['big model is changing the world']}\n"
     ]
    }
   ],
   "source": [
    "from speaksee import evaluation\n",
    "\n",
    "def tokenize(dict):\n",
    "    for key in dict:\n",
    "        new_sentence_list = []\n",
    "        for sentence in dict[key]:\n",
    "            a = ''\n",
    "            for token in nlp(sentence):\n",
    "                a += token.text\n",
    "                a += ' '\n",
    "            new_sentence_list.append(a.rstrip())\n",
    "        dict[key] = new_sentence_list\n",
    "\n",
    "    return dict\n",
    "\n",
    "# gts = tokenize(gts)\n",
    "# gen = tokenize(gen)\n",
    "\n",
    "gts = evaluation.PTBTokenizer.tokenize(gts)\n",
    "gen = evaluation.PTBTokenizer.tokenize(gen)\n",
    "\n",
    "\n",
    "print(gts,gen)\n",
    "# val_bleu, _ = evaluation.Bleu(n=4).compute_score(gts, gen)\n",
    "# method = ['Blue_1', 'Bleu_2', 'Bleu_3', 'Bleu_4']\n",
    "# metric_dict = {}\n",
    "# for metric, score in zip(method, val_bleu):\n",
    "#     metric_dict[metric] = {'entire': score * 100}\n",
    "#     print('%s %.2f' % (metric, score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf07bc0c-3c9e-402a-bf0d-449519e59481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['today is a nice day'] ['today is a nice days']\n",
      "['i like huggingface it is easy to use'] ['i like huggingface it is easy to use']\n",
      "['big model is changing the worlds'] ['big model is changing the world']\n",
      "NIST 3.85\n",
      "BLEU 85.32\n"
     ]
    }
   ],
   "source": [
    "from eval_metric import BLEUScore, NISTScore\n",
    "\n",
    "bleu = BLEUScore()\n",
    "nist = NISTScore()\n",
    "\n",
    "for sents_ref, sent_sys in zip(gts.values(), gen.values()):\n",
    "    print(sents_ref, sent_sys)\n",
    "    \n",
    "    bleu.append(sent_sys[0], sents_ref)\n",
    "    nist.append(sent_sys[0], sents_ref)\n",
    "    \n",
    "print(\"NIST %.2f\" % (nist.score()))\n",
    "print(\"BLEU %.2f\" % (bleu.score() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "fa67331a-7bc0-41bb-91f3-faefb90b26e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Today', 'is', 'a', 'nice', 'day']]\n",
      "[['I', 'like', 'huggingface,', 'it', 'is', 'easy', 'to', 'use']]\n",
      "[['big', 'model', 'is', 'changing', 'the', 'world']]\n",
      "[0.7745966692414834, 0.6976750600527641, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bigram': 0.8240905764314158,\n",
       " 'trigram': 0.7785575236143242,\n",
       " 'quagram': 0.7311587009123848}"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fast_bleu import BLEU, SelfBLEU\n",
    "import numpy as np\n",
    "\n",
    "evaluator_bleu(text, reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "305be2c2-1146-4c99-9b7d-2814cf38bf73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['big', 'model', 'is', 'changing', 'the', 'world']"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference[\"3\"][0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1343487-8351-4155-a73b-0b7f7bf65394",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,\"mask\",\"mask\",5,6,7,8,\"mask\",10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b4cb2f76-65b7-43c8-8df0-32d891a2b007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1, 2]\n",
      "[1, 2, 3]\n",
      "[1, 2, 3, 'mask']\n",
      "[1, 2, 3, 'mask']\n",
      "[1, 2, 3, 'mask', 5]\n",
      "[1, 2, 3, 'mask', 5, 6]\n",
      "[1, 2, 3, 'mask', 5, 6, 7]\n",
      "[1, 2, 3, 'mask', 5, 6, 7, 8]\n",
      "[1, 2, 3, 'mask', 5, 6, 7, 8, 'mask']\n",
      "[1, 2, 3, 'mask', 5, 6, 7, 8, 'mask', 10]\n"
     ]
    }
   ],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c16fb73f-9c97-4e65-8ea4-99e6eb15d13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 'mask', 5, 6, 7, 8, 'mask', 10]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d5ad63eb-f138-4a44-a1a1-bdfdb838d794",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (449295825.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [81]\u001b[0;36m\u001b[0m\n\u001b[0;31m    a 6\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "a 6\n",
    "if a%6 < 6:\n",
    "    print(\"hello\")\n",
    "elif a %6==1:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9055129d-d955-43cf-8226-dd0b6c07a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_set_input_ids =[1,2,3,4,5,6,7,7,9,0]\n",
    "length = len(concept_set_input_ids)\n",
    "sentinal_count = max(int(length * 0.2), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4d50ebcc-733a-4c8b-8bca-c3947e34a6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentinal_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ea03acdf-f2cd-4686-b535-9559ee754a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1026dbfe-1b87-4ece-9560-1fb1822736e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py39]",
   "language": "python",
   "name": "conda-env-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
