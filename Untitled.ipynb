{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5c27bbae-fdcf-4e31-ba4c-5e39548022f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import transformers\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    BertTokenizer,\n",
    "    GPT2Tokenizer\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
    "from transformers import GPT2LMHeadModel, AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "model_name_or_path = \"/home2/zhanghanqing/pretrained_model/gpt2/large\"\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "            model_name_or_path)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "60095695-acb4-4098-b281-acd0b091744a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[31373,   995, 31373,   995, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "source_line=\"hello world\"\n",
    "tgt_line = \"hello world\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "res=tokenizer.encode_plus(source_line,tgt_line,max_length=20,return_tensors=\"pt\",truncation=False,padding=\"max_length\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "470586ca-e491-41ea-ba8d-876a9ac986ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Includes: hello world today whether #\n",
      "[42986, 25, 23748, 995, 1909, 1771, 1303]\n"
     ]
    }
   ],
   "source": [
    "concept_set = \"hello world today whether\"\n",
    "print(f\"Includes: {concept_set} #\")\n",
    "concept_set_input_ids = tokenizer(f\"Includes: {concept_set} #\", return_tensors=\"np\")['input_ids'][0].tolist()\n",
    "print(concept_set_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "887f991c-2150-4f64-aff5-29725eff843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_prefix_space=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "9f02cd12-d9dd-4957-836f-adbafec97b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 23748, 995, 11, 220, 220, 220, 23748, 995]\n",
      "[23748, 995, 11, 220, 220, 220, 23748, 995]\n",
      "[23748, 995, 11, 220, 220, 220, 23748, 995]\n",
      "[31373, 995, 11, 220, 220, 220, 23748, 995]\n"
     ]
    }
   ],
   "source": [
    "c = tokenizer.encode(\" hello world,    hello world\")\n",
    "print(c)\n",
    "c = tokenizer.encode(\"hello world,    hello world\")\n",
    "print(c)\n",
    "tokenizer.add_prefix_space=False\n",
    "\n",
    "c = tokenizer.encode(\" hello world,    hello world\")\n",
    "print(c)\n",
    "c = tokenizer.encode(\"hello world,    hello world\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9731bbb4-24c3-4646-9f80-ddc677c23981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world,    hello world'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3065b38a-31e1-442b-ac36-0c517a4599b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 4, 5, 6, 10, 12]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,4,5,6,2,2,10,12]\n",
    "b= list(filter(lambda x: x!=2, a))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "add358f1-df80-47fe-8cb1-3667b69712dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_examples(filepath, split):\n",
    "        # TODO: This method handles input defined in _split_generators to yield (key, example) tuples from the dataset.\n",
    "        # The `key` is for legacy reasons (tfds) and is not important in itself, but must be unique for each example.\n",
    "        data = json.load(open(filepath,\"r\"))\n",
    "        for key, row in enumerate(data):\n",
    "\n",
    "            # Yields examples as (key, example) tuples\n",
    "            yield key, {\n",
    "                \"context\": row[\"Context\"],\n",
    "                \"keywords\": row[\"Words\"],\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6200c748-7983-49fb-b0a4-88bacdecd3e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean gutter house\n",
      "They had noticed water streaming down the sides of their external walls every time it rained heavily. They were sure they could fix the problem themselves but weren't quite certain of what to do so they called up a local tradesman for advice. He came up with a number of potential reasons as to why they might be having the issue and they decided to ask him round for a quote to check on the most obvious solutions.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data = json.load(open(\"../data/c2gen.json\",\"r\"))\n",
    "for r in data[\"rows\"]:\n",
    "    keywords = ' '.join(r[\"row\"][\"keywords\"])\n",
    "    context =  r[\"row\"][\"context\"]\n",
    "    print(keywords)\n",
    "    print(context)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "baa5aab1-f9da-4563-a034-dcdda0fe6c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 1920]\n",
      "[1, 2, 999, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 999, 999, 17, 18, 1920]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "concept_set_input_ids = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,1920]\n",
    "length = len(concept_set_input_ids)\n",
    "sentinal_count = max(int(length * 0.2), 1)\n",
    "sentinal_positions = random.sample(range(1, length-1), sentinal_count)\n",
    "\n",
    "cc = deepcopy(concept_set_input_ids)\n",
    "\n",
    "for i in sentinal_positions:\n",
    "    cc[i] = 999\n",
    "\n",
    "print(concept_set_input_ids)\n",
    "print(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "57ca5ea4-c305-421b-aba1-fdf199f96496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home2/zhanghanqing/formatted_wikipedia/wiki**.format\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path=\"/home2/zhanghanqing/formatted_wikipedia/\"\n",
    "print(os.path.join(path,'wiki**.format'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0f2dd42c-5fdf-46b3-b4db-f8fc0dc9f36a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m  filenames[:\u001b[38;5;241m2\u001b[39m]:\n\u001b[0;32m----> 4\u001b[0m     data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mencode(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m Path(path)\u001b[38;5;241m.\u001b[39mopen()\u001b[38;5;241m.\u001b[39mreadlines()]\n",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m  filenames[:\u001b[38;5;241m2\u001b[39m]:\n\u001b[0;32m----> 4\u001b[0m     data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m Path(path)\u001b[38;5;241m.\u001b[39mopen()\u001b[38;5;241m.\u001b[39mreadlines()]\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2257\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2220\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   2221\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   2222\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2240\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   2241\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   2242\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2243\u001b[0m \u001b[38;5;124;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   2244\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2255\u001b[0m \u001b[38;5;124;03m            method).\u001b[39;00m\n\u001b[1;32m   2256\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2257\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2266\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2267\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2663\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2653\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2654\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2655\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2656\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2660\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2661\u001b[0m )\n\u001b[0;32m-> 2663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2666\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2681\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2682\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/tokenization_utils.py:649\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    642\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 649\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[1;32m    653\u001b[0m     first_ids,\n\u001b[1;32m    654\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    668\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    669\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/tokenization_utils.py:616\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 616\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/tokenization_utils.py:547\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 547\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    548\u001b[0m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/models/gpt2/tokenization_gpt2.py:299\u001b[0m, in \u001b[0;36mGPT2Tokenizer._tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03m\"\"\"Tokenize a string.\"\"\"\u001b[39;00m\n\u001b[1;32m    298\u001b[0m bpe_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 299\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    300\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbyte_encoder[b] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m token\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    302\u001b[0m     )  \u001b[38;5;66;03m# Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     bpe_tokens\u001b[38;5;241m.\u001b[39mextend(bpe_token \u001b[38;5;28;01mfor\u001b[39;00m bpe_token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbpe(token)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/regex/regex.py:338\u001b[0m, in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags, pos, endpos, overlapped, concurrent, timeout, ignore_unused, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m\"\"\"Return a list of all matches in the string. The matches may be overlapped\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mif overlapped is True. If one or more groups are present in the pattern,\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03mreturn a list of groups; this will be a list of tuples if the pattern has\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03mmore than one group. Empty matches are included in the result.\"\"\"\u001b[39;00m\n\u001b[1;32m    337\u001b[0m pat \u001b[38;5;241m=\u001b[39m _compile(pattern, flags, ignore_unused, kwargs, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverlapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcurrent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "data = []\n",
    "for path in  filenames[:2]:\n",
    "    data += [tokenizer.encode(x) for x in Path(path).open().readlines()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "43bf8dc0-9625-47c6-ac2b-afd69b12aa69",
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function <lambda> at 0x7ff9645c58b0>: attribute lookup <lambda> on __main__ failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [62]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Pool(\u001b[38;5;241m32\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m----> 8\u001b[0m         data \u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/multiprocessing/pool.py:870\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 870\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/multiprocessing/pool.py:537\u001b[0m, in \u001b[0;36mPool._handle_tasks\u001b[0;34m(taskqueue, put, outqueue, pool, cache)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     \u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    539\u001b[0m     job, idx \u001b[38;5;241m=\u001b[39m task[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/multiprocessing/connection.py:211\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_writable()\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_bytes(\u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/multiprocessing/reduction.py:51\u001b[0m, in \u001b[0;36mForkingPickler.dumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdumps\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     50\u001b[0m     buf \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mgetbuffer()\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <function <lambda> at 0x7ff9645c58b0>: attribute lookup <lambda> on __main__ failed"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    with Pool(32) as p:\n",
    "        data =list(p.imap(lambda x: tokenizer.encode(x),  Path(filenames[0]).open().readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1318e2fc-14a9-4abd-a754-ee314350f102",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m32\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m, data))\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "len(data)\n",
    "data = list(filter(lambda x: 32 <= len(x) <= 512, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2000e6fd-0c7a-431e-a4ca-926e4b289139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90767551-06e5-41e5-ac5e-26f342948791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e7c28b6f-7c97-4faa-99e1-d1334578008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_square_subsequent_mask(sz, device='cpu'):\n",
    "        r\"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "        \"\"\"\n",
    "        \n",
    "        return torch.triu(torch.full((sz, sz), True, device=device), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e12cf2c8-5274-4738-9478-bd6905ea8ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True],\n",
       "        [False, False,  True,  True],\n",
       "        [False, False, False,  True],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_generate_square_subsequent_mask(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11895c99-5d99-49dc-a242-86e0001c5a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8, batch_first=True)\n",
    "memory = torch.rand(20, 10, 512)\n",
    "tgt = torch.rand(20, 32, 512)\n",
    "out = decoder_layer(tgt, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97595a58-5f04-40b0-90f1-26f907942b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhanghanqing/anaconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0.dev20221024+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "177affec-9c42-40d6-8724-6a1b7b587bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f9ddd3ce-2a55-4e1a-8c23-1169f98d81c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_params_simility(x, key_value, mask_value):\n",
    "\n",
    "    key_value = torch.transpose(key_value,-1,-2)\n",
    "    \n",
    "    simility_matrix = x.bmm(key_value) # b*seq_x*seq_k_v\n",
    "    \n",
    "    # simility_matrix = - torch.mean(simility_matrix, dim=-1, keepdim =False)\n",
    "    simility_matrix = - torch.max(simility_matrix, dim=-1, keepdim= False).values\n",
    "\n",
    "    \n",
    "    print(\"simility_matrix:\", simility_matrix)\n",
    "    mask = torch.zeros(x.shape[0],x.shape[1]).to(x.device)\n",
    "    mask.masked_fill_(mask_value == 1, -1000) # mask the pading token, simility to 1 direction\n",
    "\n",
    "    # out = -torch.cosine_similarity(x, key_value, dim=-1) + mask # batch*seq\n",
    "    softmax =  F.softmax((simility_matrix+mask)/0.1, dim=1) # batch*seq\n",
    "    \n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4687a1a-250e-468a-856b-68a09abaf6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = \"The school band played their instruments in unison rendering a sweet music art bot.\"\n",
    "concept = \"band instrument music sweet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1cb0cd09-ba72-4d30-955c-678c673a134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_set_input_ids = tokenizer(\"<|endoftext|> \" + concept, return_tensors=\"np\")['input_ids'][0][1:].tolist()\n",
    "inputs =tokenizer(\"<|endoftext|> \" + inputs, return_tensors=\"np\")['input_ids'][0][1:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e1c6f9f-4d92-4317-961c-a5607b84c9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simility_matrix: tensor([[0.1058, 0.1622, 0.0149, 0.1487]], grad_fn=<NegBackward0>)\n",
      "simility_matrix: tensor([[-0.1954, -0.0839, -0.1973, -0.0762]], grad_fn=<NegBackward0>)\n",
      "simility_matrix: tensor([[-3.2249, -0.6035, -0.6969, -0.1126]], grad_fn=<NegBackward0>)\n",
      "simility_matrix: tensor([[-3.2249, -0.6035, -0.6969, -0.1478]], grad_fn=<NegBackward0>)\n",
      "simility_matrix: tensor([[-3.2249, -0.6035, -0.6969, -0.1478]], grad_fn=<NegBackward0>)\n",
      "simility_matrix: tensor([[-3.2249, -2.7667, -0.6969, -0.1478]], grad_fn=<NegBackward0>)\n",
      "simility_matrix: tensor([[-3.2249, -2.7667, -0.6969, -0.1478]], grad_fn=<NegBackward0>)\n",
      "simility_matrix: tensor([[-3.2249, -2.7667, -0.6969, -0.1701]], grad_fn=<NegBackward0>)\n",
      "simility_matrix: tensor([[-3.2249, -2.7667, -0.6969, -0.1701]], grad_fn=<NegBackward0>)\n",
      "simility_matrix: tensor([[-3.2249, -2.7667, -0.6969, -0.1701]], grad_fn=<NegBackward0>)\n",
      "simility_matrix: tensor([[-3.2249, -2.7667, -0.6969, -2.8439]], grad_fn=<NegBackward0>)\n",
      "simility_matrix: tensor([[-3.2249, -2.7667, -2.4087, -2.8439]], grad_fn=<NegBackward0>)\n",
      "simility_matrix: tensor([[-3.2249, -2.7667, -2.4087, -2.8439]], grad_fn=<NegBackward0>)\n",
      "simility_matrix: tensor([[-3.2249, -2.7667, -2.4087, -2.8439]], grad_fn=<NegBackward0>)\n",
      "simility_matrix: tensor([[-3.2249, -2.7667, -2.4087, -2.8439]], grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "query = embeddings(torch.tensor(concept_set_input_ids)).unsqueeze(0)\n",
    "mask_value = (torch.tensor(concept_set_input_ids) == tokenizer.eos_token_id).long().unsqueeze(0)\n",
    "\n",
    "for i  in range(len(inputs)):\n",
    "    generated_sentence = inputs[:i+1]\n",
    "    \n",
    "    k_v = embeddings(torch.tensor(generated_sentence)).unsqueeze(0) # batch*seq*dim    \n",
    "    \n",
    "    attn_output = non_params_simility(query, k_v, mask_value).unsqueeze(1)# batch*1*seq; the bigger the value is, the more attention will be allocated\n",
    "    \n",
    "    # print(attn_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "38767edf-3ff1-40e2-912e-56d23f991b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_pad = model.get_input_embeddings()(torch.tensor([111]*10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "56879b8f-6ade-4080-84ea-63a96fd5dcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.9242, 1.7480, 2.5877, 1.9316, 3.4675, 2.8824, 1.5688, 1.3979, 2.9830,\n",
      "        3.1336])\n"
     ]
    }
   ],
   "source": [
    "max_grad = torch.rand(10,5)\n",
    "print(max_grad.sum(dim=-1))\n",
    "max_flag = (max_grad.sum(dim=-1)>1.0).unsqueeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aea8f157-7a09-4718-b9a3-0245502ef97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_flag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff824a8-fd50-4f8c-bdc9-eead7f832369",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TransformerDecoderLayer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4f5a950c14b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m  \u001b[0;32mimport\u001b[0m \u001b[0mTransformerDecoderLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TransformerDecoderLayer'"
     ]
    }
   ],
   "source": [
    "from transformers  import TransformerDecoderLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfe92948-7b87-4cbb-b541-835f5d1af2a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch.nn.TransformerDecoderLayer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7ce226d5d352>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformerDecoderLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch.nn.TransformerDecoderLayer'"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.TransformerDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07d6387f-0f8b-4f76-8651-5aeb7a8ea634",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(3,6,10)\n",
    "b = torch.rand(3,6,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb8188cb-4b30-4d4d-8419-a29772365ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a*b\n",
    "c = c.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb5d2723-6956-4779-912d-191299a8e9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.9895, 3.0855, 2.5744, 1.3559, 2.3048, 2.0286],\n",
       "        [2.3945, 1.3978, 3.4414, 3.2178, 3.6062, 2.5901],\n",
       "        [2.2273, 1.8981, 2.3170, 2.2924, 1.6941, 2.0867]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "aba45bde-f2b3-4459-bdc5-72cfee069f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8848\n",
      "6129\n",
      "50256\n",
      "{'input_ids': tensor([[50256,  8848,  6129,   384,   363,   724]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_tokens_to_ids('Ġ'+\"boat\"))\n",
    "print(tokenizer.convert_tokens_to_ids('Ġ'+\"fly\"))\n",
    "print(tokenizer.convert_tokens_to_ids('Ġ'+\"seagull\"))\n",
    "\n",
    "inputs = tokenizer(\"<|endoftext|> boat fly seagull\", return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "970627b3-9022-4190-b8b8-ed8a6f158bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_flag = torch.tensor([[1,0,1,0],[0,1,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c977b0f-8105-4b1c-a6bc-0353f50cacab",
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fd949743-b9d9-4286-941f-449938d8b10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0],\n",
      "        [0, 2],\n",
      "        [1, 1],\n",
      "        [1, 2]])\n"
     ]
    }
   ],
   "source": [
    "concept_index = mention_flag.nonzero()\n",
    "print(concept_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2d1a73b-b784-4676-835d-d6d94f671c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from info_nce import InfoNCE\n",
    "import torch\n",
    "\n",
    "loss = InfoNCE(negative_mode='paired')\n",
    "batch_size, num_negative, embedding_size = 32, 6, 128\n",
    "query = torch.randn(batch_size, embedding_size)\n",
    "positive_key = torch.randn(batch_size, embedding_size)\n",
    "negative_keys = torch.randn(batch_size, num_negative, embedding_size)\n",
    "output = loss(query, positive_key, negative_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46317fcd-ad99-4696-94db-13fc1e738086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3818)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "975b1cfe-7af2-4455-bfa9-edd5c6de8a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=a.replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a89f9f7c-3c44-45f7-98ac-eadc9129ac0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.Tensor([0, 10, 3, 0]) \n",
    "a = torch.multinomial(weights, 1, replacement=True)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "285d569b-f05e-4ab6-bf2f-d7351392e4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1],\n",
      "         [2],\n",
      "         [3],\n",
      "         [1]]]) torch.Size([1, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a= torch.tensor([[[0,1],[0,0],[2,1],[0,1]],\n",
    "                 [[1,1],[0,1],[6,1],[0,1]],\n",
    "                 [[9,1],[7,1],[0,1],[1,1]],\n",
    "                 [[8,1],[1,2],[1,3],[0,0]]])\n",
    "\n",
    "c = torch.tensor([1,2,3,1])\n",
    "c  =c.reshape(1,-1,1)\n",
    "print(c, c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e810a6f1-052c-4955-b42e-fe0a5648113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5]\n",
    "b =[1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b773e741-8554-4a88-821a-fc1c121c37a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[-2:]=b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "81af0629-9fd3-4544-8fb5-5dbe7f6acf52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 1, 2, 3]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d10645a8-0243-4de4-8334-0f9dfd8ace05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4]) torch.Size([2, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [2, 4], got [2, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-f1f2675d802f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# cross_entropy的实现\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [2, 4], got [2, 3]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input = torch.tensor([[[0.5546, 0.1304, 0.9288,0.9288],\n",
    "                        [0.6879, 0.3553, 0.9984,0.9288],\n",
    "                        [0.1474, 0.6745, 0.8948,0.9288]],\n",
    "\t\t               [[0.8524, 0.2278, 0.6476,0.9288],\n",
    "                        [0.6203, 0.6977, 0.3352,0.9288],\n",
    "                        [0.4946, 0.4613, 0.6882,0.9288]]])\n",
    "target = torch.tensor([[0, 0, 0],\n",
    "                       [0, 0, 1]])\n",
    "\n",
    "\n",
    "print(input.shape, target.shape)\n",
    "# cross_entropy的实现               \n",
    "loss = F.cross_entropy(input, target)\n",
    "print(loss)\n",
    "\n",
    "# 利用nll_loss实现cross_entropy\n",
    "input = F.softmax(input, dim=1)\n",
    "input = torch.log(input)\n",
    "# input = F.log_softmax(input, dim=1)  # 上面的两行代码和这个是等价的\n",
    "loss = F.nll_loss(input, target)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "981834f7-a33b-4111-bfa3-9a3e513bc5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5]) torch.Size([3])\n",
      "tensor(1.5943, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhanghanqing/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# input is of size N x C = 3 x 5\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.tensor([1, 0, 4])\n",
    "\n",
    "print(input.shape, target.shape)\n",
    "output = F.nll_loss(F.log_softmax(input), target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f45fce8-f200-4e2e-9750-c1a54b43c244",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'English' object has no attribute 'tagger'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m nlp\u001b[38;5;241m.\u001b[39mpipeline \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtagger\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtagger\u001b[49m)]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(_list):\n\u001b[1;32m      6\u001b[0m     new_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'English' object has no attribute 'tagger'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.pipeline = [('tagger', nlp.tagger)]\n",
    "\n",
    "def tokenize(_list):\n",
    "    new_dict = {}\n",
    "    for item in _list:\n",
    "        if isinstance(item, list):\n",
    "            new_sentence_list = []\n",
    "            for sentence in item:\n",
    "                a = ''\n",
    "                for token in nlp(sentence):\n",
    "                    a += token.text\n",
    "                    a += ' '\n",
    "                new_sentence_list.append(a.rstrip())\n",
    "            new_dict[len(new_dict)] = new_sentence_list\n",
    "        else:\n",
    "            a = ''\n",
    "            for token in nlp(item):\n",
    "                a += token.text\n",
    "                a += ' '\n",
    "            new_dict[len(new_dict)] = [a]\n",
    "\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c12670de-c7ef-4561-99e2-3799a5c12e0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIDEr 0.00\n"
     ]
    }
   ],
   "source": [
    "from speaksee import evaluation\n",
    "\n",
    "# gts = [['this', 'is', 'a', 'test'], ['this', 'is' 'test']]\n",
    "# gen = ['this', 'is', 'a', 'test']\n",
    "\n",
    "gts = {\"image1\":\"this is a test\",\"image2\":\"this is a test\"}\n",
    "gen = {\"image1\":\"this is a test\",\"image2\":\"this is a test\"}\n",
    "\n",
    "# val_meteor, _ = evaluation.Meteor().compute_score(gts, gen)\n",
    "# print('METEOR %.2f' % (val_meteor * 100))\n",
    "\n",
    "val_cider, _ = evaluation.Cider().compute_score(gts, gen)\n",
    "print('CIDEr %.2f' % (val_cider * 100))\n",
    "\n",
    "# val_spice, _ = evaluation.Spice().compute_score(gts, gen)\n",
    "# print('SPICE %.2f' % (val_spice * 100))\n",
    "# # score = sentence_bleu(reference, candidate)\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59d54e47-cd6a-4fdd-930c-aa114dff4d74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhanghanqing/anaconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/zhanghanqing/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/zhanghanqing/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/zhanghanqing/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "meteor = evaluate.load('meteor')\n",
    "predictions = [\"It is a guide to action which ensures that the military always obeys the commands of the party\",\"hello\"]\n",
    "references = [['It is a guide to action that ensures that the military will forever heed Party commands', 'It is the guiding principle which guarantees the military forces always being under the command of the Party', 'It is the practical guide for the army always to heed the directions of the party'],[\"hello hello\"]]\n",
    "results = meteor.compute(predictions=predictions, references=references)\n",
    "print(round(results['meteor'], 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92203230-2b0d-4a9d-b6a6-35b0bc252c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
    "references = [[\"hello there general kenobi\", \"hello there !\"],[\"foo bar foobar\"]]\n",
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "668297d0-564a-4fe9-ad13-0ba9aa4c797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def ZhiShu(num):\n",
    "    \n",
    "    count= 0\n",
    "    for i in  range(2, int(num/2)):\n",
    "        \n",
    "        if num%(i+1) == 0:\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "    \n",
    "    \n",
    "            \n",
    "def ZhuYingShu(num):\n",
    "    \n",
    "    if ZhiShu(num):\n",
    "        print(num)\n",
    "    else:\n",
    "        for i in  range(2, int(math.sqrt(num))):\n",
    "            if num % i ==0:\n",
    "                if ZhiShu(i):\n",
    "                    print(i)\n",
    "                    return ZhuYingShu(int(num/i))\n",
    "                else:\n",
    "                    return\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c3296eb-cdc5-458b-867d-8405f7f068c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "ZhuYingShu(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37d54fc6-7083-447d-a665-0b4fa9983d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ZhiShu(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "743012f1-e358-47ab-9530-557f832366e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 6 "
     ]
    }
   ],
   "source": [
    "def isZS(x):\n",
    "    flag = 1\n",
    "    for i in range(2, int(x**0.5)):\n",
    "        if x%i == 0:\n",
    "            flag = 0\n",
    "            print(str(i),end=' ')\n",
    "            isZS(int(x/i))\n",
    "            continue\n",
    "    if flag == 1:\n",
    "        print(str(x), end = ' ')        \n",
    " \n",
    "isZS(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca7b38ca-63cc-480b-aef1-3b83de6706b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1622776601683795"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8763179b-0eef-42dc-9b6b-513b1d0a98e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py39]",
   "language": "python",
   "name": "conda-env-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
