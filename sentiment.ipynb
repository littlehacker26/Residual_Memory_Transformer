{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "385ab244-3fae-4e2a-91bf-b21afcd9d1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhanghanqing/anaconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import pipeline, set_seed\n",
    "from eval_metric import *\n",
    "\n",
    "def distinctness(generations_data):\n",
    "    dist1, dist2, dist3 = [], [], []\n",
    "    total_words = 0\n",
    "    unigrams, bigrams, trigrams = set(), set(), set()\n",
    "    \n",
    "    for gen in generations_data:\n",
    "            o = gen.split(' ')\n",
    "            total_words += len(o)\n",
    "            unigrams.update(o)\n",
    "            for i in range(len(o) - 1):\n",
    "                bigrams.add(o[i] + '_' + o[i+1])\n",
    "            for i in range(len(o) - 2):\n",
    "                trigrams.add(o[i] + '_' + o[i+1] + '_' + o[i+2])\n",
    "                \n",
    "    if total_words == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    \n",
    "    dist1 = len(unigrams) / total_words\n",
    "    dist2 = len(bigrams) / total_words\n",
    "    dist3 = len(trigrams) / total_words\n",
    "    \n",
    "    return dist1, dist2, dist3\n",
    "\n",
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de72917e-2eac-4e3c-9e0e-bb77190aded4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./eval/generated_result_negative_seed_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdb6d1ab-0024-42c8-8146-90cd0c810515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "text_ = data[\"text\"].tolist()#[-2500:]\n",
    "print(len(text_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81e0d8ec-9da7-44da-afe8-4513bf425e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381 5000 0.0762\n"
     ]
    }
   ],
   "source": [
    "res_ = classifier(text_)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text_),count/len(text_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9912ba42-2613-4626-9390-ab22017936f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_metric import *\n",
    "\n",
    "gts={}\n",
    "for index,  d in enumerate(text_):\n",
    "    gts[index] = [d]\n",
    "    \n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed6b7ed0-735d-48b2-86e4-0b49b1a33962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.120273246662972"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9407e87a-9838-4000-972c-0f0570e07c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhanghanqing/anaconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da4a885a-b11a-40b8-b417-1b302b2cd934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0.dev20221024+cu117\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea66fc58-26c2-4d1a-b65c-ea71e53d3141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1764 2500 0.7056\n",
      "4892 5000 0.9784\n",
      "average ppl is: 41.772421006310736\n",
      "0.1421707980999632 0.5522540303354035 0.7854913961166338\n",
      "#########################\n",
      "1722 2500 0.6888\n",
      "4885 5000 0.977\n",
      "average ppl is: 41.759775576446316\n",
      "0.1427256431374449 0.5546303473668911 0.7876859163800204\n",
      "#########################\n",
      "1736 2500 0.6944\n",
      "4883 5000 0.9766\n",
      "average ppl is: 42.03815039880227\n",
      "0.14241210772097074 0.5558929565196351 0.7889194028406203\n",
      "#########################\n",
      "1725 2500 0.69\n",
      "4883 5000 0.9766\n",
      "average ppl is: 41.98135309111513\n",
      "0.14347325003032876 0.5559687007157589 0.7867099357030207\n",
      "#########################\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./eval/attribute/neg_pos_result_20_seed_41.csv\")\n",
    "\n",
    "text_ = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text_)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text_),count/len(text_))\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"./eval/attribute/neu_pos_result_20_seed_41.csv\")\n",
    "\n",
    "text = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text),count/len(text))\n",
    "\n",
    "text = text_ + text\n",
    "gts={}\n",
    "for index,  d in enumerate(text):\n",
    "    gts[index] = [d]\n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(\"average ppl is:\",ppls)\n",
    "\n",
    "text = [i.replace('\\n\\n',' ') for i in text]\n",
    "dist1, dist2, dist3 = distinctness(text)\n",
    "print(dist1, dist2, dist3)\n",
    "\n",
    "print(\"#########################\")\n",
    "data = pd.read_csv(\"./eval/attribute/neg_pos_result_20_seed_42.csv\")\n",
    "\n",
    "text_ = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text_)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text_),count/len(text_))\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"./eval/attribute/neu_pos_result_20_seed_42.csv\")\n",
    "\n",
    "text = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text),count/len(text))\n",
    "\n",
    "text = text_ + text\n",
    "gts={}\n",
    "for index,  d in enumerate(text):\n",
    "    gts[index] = [d]\n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(\"average ppl is:\",ppls)\n",
    "\n",
    "text = [i.replace('\\n\\n',' ') for i in text]\n",
    "dist1, dist2, dist3 = distinctness(text)\n",
    "print(dist1, dist2, dist3)\n",
    "\n",
    "print(\"#########################\")\n",
    "\n",
    "data = pd.read_csv(\"./eval/attribute/neg_pos_result_20_seed_43.csv\")\n",
    "\n",
    "text_ = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text_)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text_),count/len(text_))\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"./eval/attribute/neu_pos_result_20_seed_43.csv\")\n",
    "\n",
    "text = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text),count/len(text))\n",
    "\n",
    "text = text_ + text\n",
    "gts={}\n",
    "for index,  d in enumerate(text):\n",
    "    gts[index] = [d]\n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(\"average ppl is:\",ppls)\n",
    "\n",
    "text = [i.replace('\\n\\n',' ') for i in text]\n",
    "dist1, dist2, dist3 = distinctness(text)\n",
    "print(dist1, dist2, dist3)\n",
    "\n",
    "print(\"#########################\")\n",
    "data = pd.read_csv(\"./eval/attribute/neg_pos_result_20_seed_44.csv\")\n",
    "\n",
    "text_ = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text_)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text_),count/len(text_))\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"./eval/attribute/neu_pos_result_20_seed_44.csv\")\n",
    "\n",
    "text = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text),count/len(text))\n",
    "\n",
    "text = text_ + text\n",
    "gts={}\n",
    "for index,  d in enumerate(text):\n",
    "    gts[index] = [d]\n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(\"average ppl is:\",ppls)\n",
    "\n",
    "text = [i.replace('\\n\\n',' ') for i in text]\n",
    "dist1, dist2, dist3 = distinctness(text)\n",
    "print(dist1, dist2, dist3)\n",
    "\n",
    "print(\"#########################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7df3da5-2479-4808-8636-c9830a439caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "582 2500 0.2328\n",
      "193 5000 0.0386\n",
      "average ppl is: 48.94158283362723\n",
      "0.14787885515531626 0.5944755878707148 0.8173700147391662\n",
      "#########################\n",
      "571 2500 0.2284\n",
      "202 5000 0.0404\n",
      "average ppl is: 49.046738956218284\n",
      "0.14630161888618665 0.5930552484830619 0.8187289655341833\n",
      "#########################\n",
      "571 2500 0.2284\n",
      "204 5000 0.0408\n",
      "average ppl is: 49.15339391978266\n",
      "0.14773568151546546 0.5950865768832322 0.8191936756943416\n",
      "#########################\n",
      "610 2500 0.244\n",
      "209 5000 0.0418\n",
      "average ppl is: 48.68139049600086\n",
      "0.14768007887115658 0.594676196931419 0.8185963398853904\n",
      "#########################\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./eval/attribute/pos_neg_result_20_seed_41.csv\")\n",
    "\n",
    "text_ = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text_)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text_),count/len(text_))\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"./eval/attribute/neu_neg_result_20_seed_41.csv\")\n",
    "\n",
    "text = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text),count/len(text))\n",
    "\n",
    "text = text_ + text\n",
    "gts={}\n",
    "for index,  d in enumerate(text):\n",
    "    gts[index] = [d]\n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(\"average ppl is:\",ppls)\n",
    "\n",
    "text = [i.replace('\\n\\n',' ') for i in text]\n",
    "dist1, dist2, dist3 = distinctness(text)\n",
    "print(dist1, dist2, dist3)\n",
    "\n",
    "print(\"#########################\")\n",
    "data = pd.read_csv(\"./eval/attribute/pos_neg_result_20_seed_42.csv\")\n",
    "\n",
    "text_ = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text_)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text_),count/len(text_))\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"./eval/attribute/neu_neg_result_20_seed_42.csv\")\n",
    "\n",
    "text = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text),count/len(text))\n",
    "\n",
    "text = text_ + text\n",
    "gts={}\n",
    "for index,  d in enumerate(text):\n",
    "    gts[index] = [d]\n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(\"average ppl is:\",ppls)\n",
    "\n",
    "text = [i.replace('\\n\\n',' ') for i in text]\n",
    "dist1, dist2, dist3 = distinctness(text)\n",
    "print(dist1, dist2, dist3)\n",
    "\n",
    "print(\"#########################\")\n",
    "\n",
    "data = pd.read_csv(\"./eval/attribute/pos_neg_result_20_seed_43.csv\")\n",
    "\n",
    "text_ = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text_)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text_),count/len(text_))\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"./eval/attribute/neu_neg_result_20_seed_43.csv\")\n",
    "\n",
    "text = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text),count/len(text))\n",
    "\n",
    "text = text_ + text\n",
    "gts={}\n",
    "for index,  d in enumerate(text):\n",
    "    gts[index] = [d]\n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(\"average ppl is:\",ppls)\n",
    "\n",
    "text = [i.replace('\\n\\n',' ') for i in text]\n",
    "dist1, dist2, dist3 = distinctness(text)\n",
    "print(dist1, dist2, dist3)\n",
    "\n",
    "print(\"#########################\")\n",
    "data = pd.read_csv(\"./eval/attribute/pos_neg_result_20_seed_44.csv\")\n",
    "\n",
    "text_ = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text_)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text_),count/len(text_))\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"./eval/attribute/neu_neg_result_20_seed_44.csv\")\n",
    "\n",
    "text = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text),count/len(text))\n",
    "\n",
    "text = text_ + text\n",
    "gts={}\n",
    "for index,  d in enumerate(text):\n",
    "    gts[index] = [d]\n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(\"average ppl is:\",ppls)\n",
    "\n",
    "text = [i.replace('\\n\\n',' ') for i in text]\n",
    "dist1, dist2, dist3 = distinctness(text)\n",
    "print(dist1, dist2, dist3)\n",
    "\n",
    "print(\"#########################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f2bdb85-4247-41e4-b5a5-1aebdb198bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364 5000 0.0728\n",
      "1090 2500 0.436\n",
      "average ppl is: 19.015896754830344\n",
      "0.12836107165835123 0.478307051725603 0.6975490731636258\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./eval/attribute/top_p/generated_result_neu_negative_seed_42.csv\")\n",
    "\n",
    "text_ = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text_)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text_),count/len(text_))\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"./eval/attribute/top_p/generated_result_pos_negative_seed_42.csv\")\n",
    "\n",
    "text = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text),count/len(text))\n",
    "\n",
    "text = text_ + text\n",
    "gts={}\n",
    "for index,  d in enumerate(text):\n",
    "    gts[index] = [d]\n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(\"average ppl is:\",ppls)\n",
    "\n",
    "text = [i.replace('\\n\\n',' ') for i in text]\n",
    "dist1, dist2, dist3 = distinctness(text)\n",
    "print(dist1, dist2, dist3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97ce71f4-1cd3-4ad2-98fb-924e141a428f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4723 5000 0.9446\n",
      "1065 2500 0.426\n",
      "average ppl is: 17.334008393562158\n",
      "0.1265374141855705 0.44883739184513854 0.6543935176519602\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./eval/attribute/top_p/generated_result_neu_positive_seed_42.csv\")\n",
    "\n",
    "text_ = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text_)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text_),count/len(text_))\n",
    "\n",
    "data = pd.read_csv(\"./eval/attribute/top_p/generated_result_neg_positive_seed_42.csv\")\n",
    "text = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text),count/len(text))\n",
    "\n",
    "text = text_ + text\n",
    "gts={}\n",
    "for index,  d in enumerate(text):\n",
    "    gts[index] = [d]\n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(\"average ppl is:\",ppls)\n",
    "\n",
    "text = [i.replace('\\n\\n',' ') for i in text]\n",
    "dist1, dist2, dist3 = distinctness(text)\n",
    "print(dist1, dist2, dist3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90ac4857-7fab-455b-9c9f-046b6af9cb7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./eval/attribute/pos_neg_result_20_seed_41.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m text_ \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      5\u001b[0m res_ \u001b[38;5;241m=\u001b[39m classifier(text_)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./eval/attribute/pos_neg_result_20_seed_41.csv\")\n",
    "\n",
    "text_ = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text_)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text_),count/len(text_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1a9de27-25a6-43a0-bbb4-59ea012e3998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1680 2500 0.672\n",
      "4881 5000 0.9762\n",
      "average ppl is: 46.08987414804446\n",
      "0.14151928311313836 0.5560409649635218 0.7872767496488895\n",
      "571 2500 0.2284\n",
      "204 5000 0.0408\n",
      "average ppl is: 49.15339391978266\n",
      "0.14773568151546546 0.5950865768832322 0.8191936756943416\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./eval/attribute/neg_pos_result_20_seed_1.csv\")\n",
    "\n",
    "text_ = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text_)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text_),count/len(text_))\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"./eval/attribute/neu_pos_result_20_seed_1.csv\")\n",
    "\n",
    "text = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text),count/len(text))\n",
    "\n",
    "text = text_ + text\n",
    "gts={}\n",
    "for index,  d in enumerate(text):\n",
    "    gts[index] = [d]\n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(\"average ppl is:\",ppls)\n",
    "\n",
    "text = [i.replace('\\n\\n',' ') for i in text]\n",
    "dist1, dist2, dist3 = distinctness(text)\n",
    "print(dist1, dist2, dist3)\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"./eval/attribute/pos_neg_result_20_seed_43.csv\")\n",
    "\n",
    "text_ = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text_)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text_),count/len(text_))\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"./eval/attribute/neu_neg_result_20_seed_43.csv\")\n",
    "text = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text),count/len(text))\n",
    "\n",
    "text = text_ + text\n",
    "gts={}\n",
    "for index,  d in enumerate(text):\n",
    "    gts[index] = [d]\n",
    "ppls = evaluator_ppl_all(gts,\"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(\"average ppl is:\",ppls)\n",
    "\n",
    "text = [i.replace('\\n\\n',' ') for i in text]\n",
    "dist1, dist2, dist3 = distinctness(text)\n",
    "print(dist1, dist2, dist3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71fb19a2-c5e0-4136-8f0f-afa7d7d3050f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12606793198937263 0.4984806016929636 0.7745336486342266\n"
     ]
    }
   ],
   "source": [
    "print(dist1, dist2, dist3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78dcc45-91e9-4857-9274-0d90d4735045",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/home/zhanghanqing/Future_decoding/model/eval/result_beta_0.2_ranking_scope_10_1.0_negative_to_positive_[24561].csv\")\n",
    "\n",
    "text_ = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text_)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text_),count/len(text_))\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"/home/zhanghanqing/Future_decoding/model/eval/result_beta_0.2_ranking_scope_10_1.0_neutral_to_positive_[24561].csv\")\n",
    "\n",
    "text = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text),count/len(text))\n",
    "\n",
    "text = text_ + text\n",
    "gts={}\n",
    "for index,  d in enumerate(text):\n",
    "    gts[index] = [d]\n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(\"average ppl is:\",ppls)\n",
    "\n",
    "text = [i.replace('\\n\\n',' ') for i in text]\n",
    "dist1, dist2, dist3 = distinctness(text)\n",
    "print(dist1, dist2, dist3)\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"/home/zhanghanqing/Future_decoding/model/eval/result_beta_0.2_ranking_scope_10_1.0_positive_to_negative_[31591].csv\")\n",
    "\n",
    "text_ = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text_)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text_),count/len(text_))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"/home/zhanghanqing/Future_decoding/model/eval/result_beta_0.2_ranking_scope_10_1.0_neutral_to_negative_[31591].csv\")\n",
    "text = data[\"text\"].tolist()\n",
    "\n",
    "res_ = classifier(text)\n",
    "count = 0\n",
    "for a in res_:\n",
    "    if a[\"label\"] == \"POSITIVE\":\n",
    "        count+=1\n",
    "print(count,len(text),count/len(text))\n",
    "\n",
    "text = text_ + text\n",
    "gts={}\n",
    "for index,  d in enumerate(text):\n",
    "    gts[index] = [d]\n",
    "ppls = evaluator_ppl_all(gts, \"/home2/zhanghanqing/pretrained_model/gpt2/large\")\n",
    "print(\"average ppl is:\",ppls)\n",
    "\n",
    "text = [i.replace('\\n\\n',' ') for i in text]\n",
    "dist1, dist2, dist3 = distinctness(text)\n",
    "print(dist1, dist2, dist3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py39]",
   "language": "python",
   "name": "conda-env-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
